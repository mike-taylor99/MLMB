{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtayl\\AppData\\Local\\Temp\\ipykernel_29144\\68269814.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "In this notebook, we will be using Gradient Boosting in order to try and predict individual games. We will be using the testing and training data that was generated from the `training-data` notebook to train our model.\n",
    "## Getting the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG_SMA</th>\n",
       "      <th>FG_CMA</th>\n",
       "      <th>FG_EMA</th>\n",
       "      <th>FGA_SMA</th>\n",
       "      <th>FGA_CMA</th>\n",
       "      <th>FGA_EMA</th>\n",
       "      <th>FG%_SMA</th>\n",
       "      <th>FG%_CMA</th>\n",
       "      <th>FG%_EMA</th>\n",
       "      <th>3P_SMA</th>\n",
       "      <th>3P_CMA</th>\n",
       "      <th>3P_EMA</th>\n",
       "      <th>3PA_SMA</th>\n",
       "      <th>3PA_CMA</th>\n",
       "      <th>3PA_EMA</th>\n",
       "      <th>3P%_SMA</th>\n",
       "      <th>3P%_CMA</th>\n",
       "      <th>3P%_EMA</th>\n",
       "      <th>FT_SMA</th>\n",
       "      <th>FT_CMA</th>\n",
       "      <th>FT_EMA</th>\n",
       "      <th>FTA_SMA</th>\n",
       "      <th>FTA_CMA</th>\n",
       "      <th>FTA_EMA</th>\n",
       "      <th>FT%_SMA</th>\n",
       "      <th>FT%_CMA</th>\n",
       "      <th>FT%_EMA</th>\n",
       "      <th>ORB_SMA</th>\n",
       "      <th>ORB_CMA</th>\n",
       "      <th>ORB_EMA</th>\n",
       "      <th>TRB_SMA</th>\n",
       "      <th>TRB_CMA</th>\n",
       "      <th>TRB_EMA</th>\n",
       "      <th>AST_SMA</th>\n",
       "      <th>AST_CMA</th>\n",
       "      <th>AST_EMA</th>\n",
       "      <th>STL_SMA</th>\n",
       "      <th>STL_CMA</th>\n",
       "      <th>STL_EMA</th>\n",
       "      <th>BLK_SMA</th>\n",
       "      <th>BLK_CMA</th>\n",
       "      <th>BLK_EMA</th>\n",
       "      <th>TOV_SMA</th>\n",
       "      <th>TOV_CMA</th>\n",
       "      <th>TOV_EMA</th>\n",
       "      <th>PF_SMA</th>\n",
       "      <th>PF_CMA</th>\n",
       "      <th>PF_EMA</th>\n",
       "      <th>ORtg_SMA</th>\n",
       "      <th>ORtg_CMA</th>\n",
       "      <th>ORtg_EMA</th>\n",
       "      <th>DRtg_SMA</th>\n",
       "      <th>DRtg_CMA</th>\n",
       "      <th>DRtg_EMA</th>\n",
       "      <th>Pace_SMA</th>\n",
       "      <th>Pace_CMA</th>\n",
       "      <th>Pace_EMA</th>\n",
       "      <th>FTr_SMA</th>\n",
       "      <th>FTr_CMA</th>\n",
       "      <th>FTr_EMA</th>\n",
       "      <th>3PAr_SMA</th>\n",
       "      <th>3PAr_CMA</th>\n",
       "      <th>3PAr_EMA</th>\n",
       "      <th>TS%_SMA</th>\n",
       "      <th>TS%_CMA</th>\n",
       "      <th>TS%_EMA</th>\n",
       "      <th>TRB%_SMA</th>\n",
       "      <th>TRB%_CMA</th>\n",
       "      <th>TRB%_EMA</th>\n",
       "      <th>AST%_SMA</th>\n",
       "      <th>AST%_CMA</th>\n",
       "      <th>AST%_EMA</th>\n",
       "      <th>STL%_SMA</th>\n",
       "      <th>STL%_CMA</th>\n",
       "      <th>STL%_EMA</th>\n",
       "      <th>BLK%_SMA</th>\n",
       "      <th>BLK%_CMA</th>\n",
       "      <th>BLK%_EMA</th>\n",
       "      <th>eFG%_SMA</th>\n",
       "      <th>eFG%_CMA</th>\n",
       "      <th>eFG%_EMA</th>\n",
       "      <th>TOV%_SMA</th>\n",
       "      <th>TOV%_CMA</th>\n",
       "      <th>TOV%_EMA</th>\n",
       "      <th>ORB%_SMA</th>\n",
       "      <th>ORB%_CMA</th>\n",
       "      <th>ORB%_EMA</th>\n",
       "      <th>FT/FGA_SMA</th>\n",
       "      <th>FT/FGA_CMA</th>\n",
       "      <th>FT/FGA_EMA</th>\n",
       "      <th>opp_FG_SMA</th>\n",
       "      <th>opp_FG_CMA</th>\n",
       "      <th>opp_FG_EMA</th>\n",
       "      <th>opp_FGA_SMA</th>\n",
       "      <th>opp_FGA_CMA</th>\n",
       "      <th>opp_FGA_EMA</th>\n",
       "      <th>opp_FG%_SMA</th>\n",
       "      <th>opp_FG%_CMA</th>\n",
       "      <th>opp_FG%_EMA</th>\n",
       "      <th>opp_3P_SMA</th>\n",
       "      <th>opp_3P_CMA</th>\n",
       "      <th>opp_3P_EMA</th>\n",
       "      <th>opp_3PA_SMA</th>\n",
       "      <th>opp_3PA_CMA</th>\n",
       "      <th>opp_3PA_EMA</th>\n",
       "      <th>opp_3P%_SMA</th>\n",
       "      <th>opp_3P%_CMA</th>\n",
       "      <th>opp_3P%_EMA</th>\n",
       "      <th>opp_FT_SMA</th>\n",
       "      <th>opp_FT_CMA</th>\n",
       "      <th>opp_FT_EMA</th>\n",
       "      <th>opp_FTA_SMA</th>\n",
       "      <th>opp_FTA_CMA</th>\n",
       "      <th>opp_FTA_EMA</th>\n",
       "      <th>opp_FT%_SMA</th>\n",
       "      <th>opp_FT%_CMA</th>\n",
       "      <th>opp_FT%_EMA</th>\n",
       "      <th>opp_ORB_SMA</th>\n",
       "      <th>opp_ORB_CMA</th>\n",
       "      <th>opp_ORB_EMA</th>\n",
       "      <th>opp_TRB_SMA</th>\n",
       "      <th>opp_TRB_CMA</th>\n",
       "      <th>opp_TRB_EMA</th>\n",
       "      <th>opp_AST_SMA</th>\n",
       "      <th>opp_AST_CMA</th>\n",
       "      <th>opp_AST_EMA</th>\n",
       "      <th>opp_STL_SMA</th>\n",
       "      <th>opp_STL_CMA</th>\n",
       "      <th>opp_STL_EMA</th>\n",
       "      <th>opp_BLK_SMA</th>\n",
       "      <th>opp_BLK_CMA</th>\n",
       "      <th>opp_BLK_EMA</th>\n",
       "      <th>opp_TOV_SMA</th>\n",
       "      <th>opp_TOV_CMA</th>\n",
       "      <th>opp_TOV_EMA</th>\n",
       "      <th>opp_PF_SMA</th>\n",
       "      <th>opp_PF_CMA</th>\n",
       "      <th>opp_PF_EMA</th>\n",
       "      <th>opp_ORtg_SMA</th>\n",
       "      <th>opp_ORtg_CMA</th>\n",
       "      <th>opp_ORtg_EMA</th>\n",
       "      <th>opp_DRtg_SMA</th>\n",
       "      <th>opp_DRtg_CMA</th>\n",
       "      <th>opp_DRtg_EMA</th>\n",
       "      <th>opp_Pace_SMA</th>\n",
       "      <th>opp_Pace_CMA</th>\n",
       "      <th>opp_Pace_EMA</th>\n",
       "      <th>opp_FTr_SMA</th>\n",
       "      <th>opp_FTr_CMA</th>\n",
       "      <th>opp_FTr_EMA</th>\n",
       "      <th>opp_3PAr_SMA</th>\n",
       "      <th>opp_3PAr_CMA</th>\n",
       "      <th>opp_3PAr_EMA</th>\n",
       "      <th>opp_TS%_SMA</th>\n",
       "      <th>opp_TS%_CMA</th>\n",
       "      <th>opp_TS%_EMA</th>\n",
       "      <th>opp_TRB%_SMA</th>\n",
       "      <th>opp_TRB%_CMA</th>\n",
       "      <th>opp_TRB%_EMA</th>\n",
       "      <th>opp_AST%_SMA</th>\n",
       "      <th>opp_AST%_CMA</th>\n",
       "      <th>opp_AST%_EMA</th>\n",
       "      <th>opp_STL%_SMA</th>\n",
       "      <th>opp_STL%_CMA</th>\n",
       "      <th>opp_STL%_EMA</th>\n",
       "      <th>opp_BLK%_SMA</th>\n",
       "      <th>opp_BLK%_CMA</th>\n",
       "      <th>opp_BLK%_EMA</th>\n",
       "      <th>opp_eFG%_SMA</th>\n",
       "      <th>opp_eFG%_CMA</th>\n",
       "      <th>opp_eFG%_EMA</th>\n",
       "      <th>opp_TOV%_SMA</th>\n",
       "      <th>opp_TOV%_CMA</th>\n",
       "      <th>opp_TOV%_EMA</th>\n",
       "      <th>opp_ORB%_SMA</th>\n",
       "      <th>opp_ORB%_CMA</th>\n",
       "      <th>opp_ORB%_EMA</th>\n",
       "      <th>opp_FT/FGA_SMA</th>\n",
       "      <th>opp_FT/FGA_CMA</th>\n",
       "      <th>opp_FT/FGA_EMA</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Win</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.8</td>\n",
       "      <td>24.653846</td>\n",
       "      <td>23.211004</td>\n",
       "      <td>54.4</td>\n",
       "      <td>56.038462</td>\n",
       "      <td>55.951135</td>\n",
       "      <td>0.4192</td>\n",
       "      <td>0.440962</td>\n",
       "      <td>0.414875</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.307692</td>\n",
       "      <td>4.723021</td>\n",
       "      <td>17.4</td>\n",
       "      <td>20.076923</td>\n",
       "      <td>17.688046</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>0.318923</td>\n",
       "      <td>0.265432</td>\n",
       "      <td>15.4</td>\n",
       "      <td>13.615385</td>\n",
       "      <td>12.571720</td>\n",
       "      <td>20.6</td>\n",
       "      <td>18.807692</td>\n",
       "      <td>16.287396</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>0.718615</td>\n",
       "      <td>0.756023</td>\n",
       "      <td>7.8</td>\n",
       "      <td>7.423077</td>\n",
       "      <td>7.630525</td>\n",
       "      <td>30.6</td>\n",
       "      <td>29.538462</td>\n",
       "      <td>29.642729</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.307692</td>\n",
       "      <td>11.694995</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.923077</td>\n",
       "      <td>6.909948</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>3.675965</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.384615</td>\n",
       "      <td>12.212031</td>\n",
       "      <td>20.2</td>\n",
       "      <td>16.769231</td>\n",
       "      <td>19.323787</td>\n",
       "      <td>96.86</td>\n",
       "      <td>100.507692</td>\n",
       "      <td>93.371986</td>\n",
       "      <td>104.72</td>\n",
       "      <td>101.753846</td>\n",
       "      <td>101.552341</td>\n",
       "      <td>68.06</td>\n",
       "      <td>68.665385</td>\n",
       "      <td>67.742130</td>\n",
       "      <td>0.3852</td>\n",
       "      <td>0.342192</td>\n",
       "      <td>0.297102</td>\n",
       "      <td>0.3228</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.318668</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>48.30</td>\n",
       "      <td>47.223077</td>\n",
       "      <td>47.416480</td>\n",
       "      <td>53.10</td>\n",
       "      <td>50.046154</td>\n",
       "      <td>50.683134</td>\n",
       "      <td>8.20</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.137945</td>\n",
       "      <td>8.38</td>\n",
       "      <td>11.403846</td>\n",
       "      <td>10.344138</td>\n",
       "      <td>0.4658</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.457629</td>\n",
       "      <td>15.76</td>\n",
       "      <td>14.819231</td>\n",
       "      <td>16.077612</td>\n",
       "      <td>25.74</td>\n",
       "      <td>23.003846</td>\n",
       "      <td>24.421789</td>\n",
       "      <td>0.2866</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.228649</td>\n",
       "      <td>21.4</td>\n",
       "      <td>21.538462</td>\n",
       "      <td>21.057987</td>\n",
       "      <td>54.8</td>\n",
       "      <td>52.538462</td>\n",
       "      <td>54.855677</td>\n",
       "      <td>0.3910</td>\n",
       "      <td>0.409423</td>\n",
       "      <td>0.384831</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.076923</td>\n",
       "      <td>8.884101</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.846154</td>\n",
       "      <td>25.275189</td>\n",
       "      <td>0.3360</td>\n",
       "      <td>0.321192</td>\n",
       "      <td>0.350582</td>\n",
       "      <td>10.4</td>\n",
       "      <td>11.846154</td>\n",
       "      <td>11.148380</td>\n",
       "      <td>14.6</td>\n",
       "      <td>15.884615</td>\n",
       "      <td>14.613588</td>\n",
       "      <td>0.7082</td>\n",
       "      <td>0.739269</td>\n",
       "      <td>0.753357</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.115385</td>\n",
       "      <td>7.383697</td>\n",
       "      <td>31.4</td>\n",
       "      <td>30.653846</td>\n",
       "      <td>32.536725</td>\n",
       "      <td>11.8</td>\n",
       "      <td>11.653846</td>\n",
       "      <td>11.233467</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.192308</td>\n",
       "      <td>3.474109</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.346154</td>\n",
       "      <td>2.342764</td>\n",
       "      <td>11.6</td>\n",
       "      <td>11.346154</td>\n",
       "      <td>13.049714</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.653846</td>\n",
       "      <td>15.671923</td>\n",
       "      <td>92.98</td>\n",
       "      <td>96.769231</td>\n",
       "      <td>93.931371</td>\n",
       "      <td>102.02</td>\n",
       "      <td>104.400000</td>\n",
       "      <td>99.376988</td>\n",
       "      <td>65.82</td>\n",
       "      <td>65.400000</td>\n",
       "      <td>66.126001</td>\n",
       "      <td>0.2672</td>\n",
       "      <td>0.316538</td>\n",
       "      <td>0.267147</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>0.459948</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>0.523346</td>\n",
       "      <td>0.502494</td>\n",
       "      <td>47.46</td>\n",
       "      <td>49.119231</td>\n",
       "      <td>49.264062</td>\n",
       "      <td>55.60</td>\n",
       "      <td>53.553846</td>\n",
       "      <td>53.411235</td>\n",
       "      <td>5.08</td>\n",
       "      <td>6.373077</td>\n",
       "      <td>5.167442</td>\n",
       "      <td>6.86</td>\n",
       "      <td>7.053846</td>\n",
       "      <td>6.029065</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.484962</td>\n",
       "      <td>0.465742</td>\n",
       "      <td>15.52</td>\n",
       "      <td>15.811538</td>\n",
       "      <td>17.140137</td>\n",
       "      <td>22.40</td>\n",
       "      <td>20.638462</td>\n",
       "      <td>24.324966</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.236269</td>\n",
       "      <td>0.203534</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>25.363636</td>\n",
       "      <td>24.750136</td>\n",
       "      <td>58.6</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>56.395143</td>\n",
       "      <td>0.4496</td>\n",
       "      <td>0.446727</td>\n",
       "      <td>0.442387</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.727273</td>\n",
       "      <td>4.457340</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.681818</td>\n",
       "      <td>15.916814</td>\n",
       "      <td>0.2764</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.282430</td>\n",
       "      <td>17.2</td>\n",
       "      <td>14.590909</td>\n",
       "      <td>17.632687</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.681818</td>\n",
       "      <td>21.997752</td>\n",
       "      <td>0.7602</td>\n",
       "      <td>0.735909</td>\n",
       "      <td>0.786252</td>\n",
       "      <td>11.4</td>\n",
       "      <td>8.272727</td>\n",
       "      <td>10.136214</td>\n",
       "      <td>34.2</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>32.159791</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.681818</td>\n",
       "      <td>10.272923</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6.499974</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.909091</td>\n",
       "      <td>2.816150</td>\n",
       "      <td>10.6</td>\n",
       "      <td>10.863636</td>\n",
       "      <td>11.167679</td>\n",
       "      <td>16.4</td>\n",
       "      <td>15.454545</td>\n",
       "      <td>15.446714</td>\n",
       "      <td>108.50</td>\n",
       "      <td>103.040909</td>\n",
       "      <td>106.421923</td>\n",
       "      <td>105.94</td>\n",
       "      <td>101.304545</td>\n",
       "      <td>105.870044</td>\n",
       "      <td>67.44</td>\n",
       "      <td>68.240909</td>\n",
       "      <td>67.298025</td>\n",
       "      <td>0.3826</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.396063</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.328182</td>\n",
       "      <td>0.284080</td>\n",
       "      <td>0.5360</td>\n",
       "      <td>0.537273</td>\n",
       "      <td>0.539165</td>\n",
       "      <td>53.86</td>\n",
       "      <td>50.554545</td>\n",
       "      <td>51.135159</td>\n",
       "      <td>42.50</td>\n",
       "      <td>45.390909</td>\n",
       "      <td>41.533877</td>\n",
       "      <td>8.56</td>\n",
       "      <td>7.936364</td>\n",
       "      <td>9.626756</td>\n",
       "      <td>10.60</td>\n",
       "      <td>8.509091</td>\n",
       "      <td>8.985101</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.482619</td>\n",
       "      <td>13.44</td>\n",
       "      <td>14.113636</td>\n",
       "      <td>14.382758</td>\n",
       "      <td>34.70</td>\n",
       "      <td>25.840909</td>\n",
       "      <td>31.393061</td>\n",
       "      <td>0.3014</td>\n",
       "      <td>0.264045</td>\n",
       "      <td>0.319346</td>\n",
       "      <td>27.8</td>\n",
       "      <td>26.869565</td>\n",
       "      <td>27.521328</td>\n",
       "      <td>60.0</td>\n",
       "      <td>58.217391</td>\n",
       "      <td>60.888683</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.462348</td>\n",
       "      <td>0.454192</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.826087</td>\n",
       "      <td>7.595725</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>23.311761</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.383913</td>\n",
       "      <td>0.288509</td>\n",
       "      <td>8.2</td>\n",
       "      <td>10.521739</td>\n",
       "      <td>7.798636</td>\n",
       "      <td>11.2</td>\n",
       "      <td>14.869565</td>\n",
       "      <td>11.644625</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.727348</td>\n",
       "      <td>0.720520</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.826087</td>\n",
       "      <td>8.042637</td>\n",
       "      <td>32.2</td>\n",
       "      <td>29.043478</td>\n",
       "      <td>32.869443</td>\n",
       "      <td>14.8</td>\n",
       "      <td>15.260870</td>\n",
       "      <td>13.426693</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.260870</td>\n",
       "      <td>5.431505</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.478261</td>\n",
       "      <td>3.540045</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.391304</td>\n",
       "      <td>10.488640</td>\n",
       "      <td>15.6</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>14.994876</td>\n",
       "      <td>108.82</td>\n",
       "      <td>106.200000</td>\n",
       "      <td>102.699579</td>\n",
       "      <td>105.02</td>\n",
       "      <td>100.552174</td>\n",
       "      <td>103.770446</td>\n",
       "      <td>67.90</td>\n",
       "      <td>69.226087</td>\n",
       "      <td>68.827918</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.254087</td>\n",
       "      <td>0.189748</td>\n",
       "      <td>0.4412</td>\n",
       "      <td>0.430783</td>\n",
       "      <td>0.389290</td>\n",
       "      <td>0.5694</td>\n",
       "      <td>0.568565</td>\n",
       "      <td>0.535933</td>\n",
       "      <td>50.22</td>\n",
       "      <td>47.143478</td>\n",
       "      <td>51.452466</td>\n",
       "      <td>52.96</td>\n",
       "      <td>57.156522</td>\n",
       "      <td>48.568758</td>\n",
       "      <td>7.34</td>\n",
       "      <td>10.417391</td>\n",
       "      <td>7.870382</td>\n",
       "      <td>7.20</td>\n",
       "      <td>9.239130</td>\n",
       "      <td>7.712390</td>\n",
       "      <td>0.5504</td>\n",
       "      <td>0.547261</td>\n",
       "      <td>0.518834</td>\n",
       "      <td>13.40</td>\n",
       "      <td>14.778261</td>\n",
       "      <td>13.688026</td>\n",
       "      <td>24.02</td>\n",
       "      <td>22.078261</td>\n",
       "      <td>24.345978</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.180522</td>\n",
       "      <td>0.128987</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.4</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>25.671618</td>\n",
       "      <td>57.8</td>\n",
       "      <td>63.100000</td>\n",
       "      <td>56.427805</td>\n",
       "      <td>0.4536</td>\n",
       "      <td>0.445450</td>\n",
       "      <td>0.451120</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.650000</td>\n",
       "      <td>5.819351</td>\n",
       "      <td>20.6</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>20.033660</td>\n",
       "      <td>0.3054</td>\n",
       "      <td>0.321850</td>\n",
       "      <td>0.288661</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.150000</td>\n",
       "      <td>9.090603</td>\n",
       "      <td>14.2</td>\n",
       "      <td>13.950000</td>\n",
       "      <td>15.245384</td>\n",
       "      <td>0.5978</td>\n",
       "      <td>0.637750</td>\n",
       "      <td>0.577958</td>\n",
       "      <td>8.4</td>\n",
       "      <td>10.150000</td>\n",
       "      <td>8.553029</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.750000</td>\n",
       "      <td>28.593819</td>\n",
       "      <td>15.6</td>\n",
       "      <td>16.050000</td>\n",
       "      <td>14.707068</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.350000</td>\n",
       "      <td>7.326814</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>5.228792</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>13.141423</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.550000</td>\n",
       "      <td>13.786065</td>\n",
       "      <td>97.50</td>\n",
       "      <td>99.430000</td>\n",
       "      <td>95.954924</td>\n",
       "      <td>107.38</td>\n",
       "      <td>102.715000</td>\n",
       "      <td>104.121329</td>\n",
       "      <td>69.56</td>\n",
       "      <td>71.955000</td>\n",
       "      <td>68.716990</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.284286</td>\n",
       "      <td>0.3574</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.358103</td>\n",
       "      <td>0.5236</td>\n",
       "      <td>0.514950</td>\n",
       "      <td>0.516993</td>\n",
       "      <td>46.38</td>\n",
       "      <td>50.190000</td>\n",
       "      <td>46.864038</td>\n",
       "      <td>57.42</td>\n",
       "      <td>54.915000</td>\n",
       "      <td>56.400907</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.125000</td>\n",
       "      <td>10.660785</td>\n",
       "      <td>13.74</td>\n",
       "      <td>10.635000</td>\n",
       "      <td>14.307917</td>\n",
       "      <td>0.5080</td>\n",
       "      <td>0.497250</td>\n",
       "      <td>0.502925</td>\n",
       "      <td>16.86</td>\n",
       "      <td>15.710000</td>\n",
       "      <td>17.228947</td>\n",
       "      <td>27.14</td>\n",
       "      <td>30.190000</td>\n",
       "      <td>27.231699</td>\n",
       "      <td>0.1584</td>\n",
       "      <td>0.147650</td>\n",
       "      <td>0.165701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.454545</td>\n",
       "      <td>25.679242</td>\n",
       "      <td>57.4</td>\n",
       "      <td>55.090909</td>\n",
       "      <td>57.564619</td>\n",
       "      <td>0.4572</td>\n",
       "      <td>0.464636</td>\n",
       "      <td>0.450118</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.545455</td>\n",
       "      <td>7.799799</td>\n",
       "      <td>21.4</td>\n",
       "      <td>22.818182</td>\n",
       "      <td>21.633975</td>\n",
       "      <td>0.3872</td>\n",
       "      <td>0.377591</td>\n",
       "      <td>0.365943</td>\n",
       "      <td>13.2</td>\n",
       "      <td>11.454545</td>\n",
       "      <td>11.706824</td>\n",
       "      <td>18.6</td>\n",
       "      <td>17.318182</td>\n",
       "      <td>17.203915</td>\n",
       "      <td>0.7188</td>\n",
       "      <td>0.644227</td>\n",
       "      <td>0.701938</td>\n",
       "      <td>8.8</td>\n",
       "      <td>9.272727</td>\n",
       "      <td>8.568962</td>\n",
       "      <td>34.8</td>\n",
       "      <td>34.318182</td>\n",
       "      <td>34.254804</td>\n",
       "      <td>15.8</td>\n",
       "      <td>16.181818</td>\n",
       "      <td>14.151697</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.954545</td>\n",
       "      <td>4.269750</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.772727</td>\n",
       "      <td>5.840287</td>\n",
       "      <td>11.2</td>\n",
       "      <td>13.681818</td>\n",
       "      <td>12.204707</td>\n",
       "      <td>17.8</td>\n",
       "      <td>14.727273</td>\n",
       "      <td>18.470674</td>\n",
       "      <td>105.80</td>\n",
       "      <td>104.645455</td>\n",
       "      <td>102.108583</td>\n",
       "      <td>100.98</td>\n",
       "      <td>95.209091</td>\n",
       "      <td>100.397883</td>\n",
       "      <td>67.80</td>\n",
       "      <td>67.559091</td>\n",
       "      <td>67.625642</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.329273</td>\n",
       "      <td>0.302344</td>\n",
       "      <td>0.3732</td>\n",
       "      <td>0.417636</td>\n",
       "      <td>0.375464</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>0.563136</td>\n",
       "      <td>0.543310</td>\n",
       "      <td>51.72</td>\n",
       "      <td>54.690909</td>\n",
       "      <td>51.240622</td>\n",
       "      <td>60.86</td>\n",
       "      <td>63.813636</td>\n",
       "      <td>55.153792</td>\n",
       "      <td>7.18</td>\n",
       "      <td>8.763636</td>\n",
       "      <td>6.169933</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.968182</td>\n",
       "      <td>14.708419</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.544273</td>\n",
       "      <td>0.518760</td>\n",
       "      <td>14.50</td>\n",
       "      <td>17.772727</td>\n",
       "      <td>15.755138</td>\n",
       "      <td>26.50</td>\n",
       "      <td>30.259091</td>\n",
       "      <td>25.897915</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.220864</td>\n",
       "      <td>0.207807</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.6</td>\n",
       "      <td>25.666667</td>\n",
       "      <td>24.534714</td>\n",
       "      <td>52.2</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>52.713938</td>\n",
       "      <td>0.4716</td>\n",
       "      <td>0.471333</td>\n",
       "      <td>0.464322</td>\n",
       "      <td>9.8</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>9.373407</td>\n",
       "      <td>23.4</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>24.219688</td>\n",
       "      <td>0.4112</td>\n",
       "      <td>0.379583</td>\n",
       "      <td>0.380740</td>\n",
       "      <td>12.8</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>13.450327</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>20.433324</td>\n",
       "      <td>0.6826</td>\n",
       "      <td>0.681417</td>\n",
       "      <td>0.683046</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>7.127945</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.666667</td>\n",
       "      <td>29.147917</td>\n",
       "      <td>14.6</td>\n",
       "      <td>13.083333</td>\n",
       "      <td>14.835013</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>6.032555</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>1.462046</td>\n",
       "      <td>13.8</td>\n",
       "      <td>14.583333</td>\n",
       "      <td>12.952757</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.583333</td>\n",
       "      <td>20.080306</td>\n",
       "      <td>105.38</td>\n",
       "      <td>100.925000</td>\n",
       "      <td>105.583149</td>\n",
       "      <td>101.46</td>\n",
       "      <td>99.975000</td>\n",
       "      <td>97.257504</td>\n",
       "      <td>68.54</td>\n",
       "      <td>70.741667</td>\n",
       "      <td>68.297726</td>\n",
       "      <td>0.3782</td>\n",
       "      <td>0.371417</td>\n",
       "      <td>0.389580</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>0.370167</td>\n",
       "      <td>0.461446</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0.568833</td>\n",
       "      <td>0.578359</td>\n",
       "      <td>46.90</td>\n",
       "      <td>48.141667</td>\n",
       "      <td>49.164642</td>\n",
       "      <td>58.10</td>\n",
       "      <td>49.541667</td>\n",
       "      <td>59.472853</td>\n",
       "      <td>8.80</td>\n",
       "      <td>7.758333</td>\n",
       "      <td>8.803756</td>\n",
       "      <td>4.10</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.202775</td>\n",
       "      <td>0.5662</td>\n",
       "      <td>0.542833</td>\n",
       "      <td>0.554087</td>\n",
       "      <td>18.10</td>\n",
       "      <td>18.691667</td>\n",
       "      <td>16.936403</td>\n",
       "      <td>23.86</td>\n",
       "      <td>22.291667</td>\n",
       "      <td>24.287561</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>0.249083</td>\n",
       "      <td>0.256014</td>\n",
       "      <td>26.2</td>\n",
       "      <td>25.928571</td>\n",
       "      <td>27.202928</td>\n",
       "      <td>56.6</td>\n",
       "      <td>58.857143</td>\n",
       "      <td>59.122634</td>\n",
       "      <td>0.4634</td>\n",
       "      <td>0.440357</td>\n",
       "      <td>0.460755</td>\n",
       "      <td>9.2</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>9.989433</td>\n",
       "      <td>25.8</td>\n",
       "      <td>24.785714</td>\n",
       "      <td>27.224526</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.379286</td>\n",
       "      <td>0.365979</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11.071429</td>\n",
       "      <td>10.378299</td>\n",
       "      <td>14.6</td>\n",
       "      <td>14.857143</td>\n",
       "      <td>13.676782</td>\n",
       "      <td>0.7270</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.767052</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>6.908517</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.857143</td>\n",
       "      <td>28.080932</td>\n",
       "      <td>17.2</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>18.015524</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>8.948778</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4.357143</td>\n",
       "      <td>5.081276</td>\n",
       "      <td>18.6</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>18.342378</td>\n",
       "      <td>19.4</td>\n",
       "      <td>17.357143</td>\n",
       "      <td>18.155262</td>\n",
       "      <td>96.16</td>\n",
       "      <td>94.407143</td>\n",
       "      <td>97.725044</td>\n",
       "      <td>103.50</td>\n",
       "      <td>108.471429</td>\n",
       "      <td>105.612118</td>\n",
       "      <td>75.26</td>\n",
       "      <td>75.585714</td>\n",
       "      <td>76.090106</td>\n",
       "      <td>0.2588</td>\n",
       "      <td>0.249857</td>\n",
       "      <td>0.229168</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.419500</td>\n",
       "      <td>0.460251</td>\n",
       "      <td>0.5706</td>\n",
       "      <td>0.547571</td>\n",
       "      <td>0.569564</td>\n",
       "      <td>45.38</td>\n",
       "      <td>46.114286</td>\n",
       "      <td>46.860574</td>\n",
       "      <td>64.50</td>\n",
       "      <td>59.807143</td>\n",
       "      <td>65.736169</td>\n",
       "      <td>11.76</td>\n",
       "      <td>9.042857</td>\n",
       "      <td>11.850792</td>\n",
       "      <td>13.44</td>\n",
       "      <td>10.992857</td>\n",
       "      <td>12.783031</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>0.520357</td>\n",
       "      <td>0.544734</td>\n",
       "      <td>22.70</td>\n",
       "      <td>21.035714</td>\n",
       "      <td>21.910853</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.192857</td>\n",
       "      <td>23.197858</td>\n",
       "      <td>0.1908</td>\n",
       "      <td>0.185571</td>\n",
       "      <td>0.173572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.8</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.409965</td>\n",
       "      <td>60.8</td>\n",
       "      <td>59.692308</td>\n",
       "      <td>60.456760</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.447385</td>\n",
       "      <td>0.435041</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.615385</td>\n",
       "      <td>7.106061</td>\n",
       "      <td>19.8</td>\n",
       "      <td>21.307692</td>\n",
       "      <td>19.692654</td>\n",
       "      <td>0.3630</td>\n",
       "      <td>0.344846</td>\n",
       "      <td>0.351888</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.461538</td>\n",
       "      <td>10.878916</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.230769</td>\n",
       "      <td>15.448029</td>\n",
       "      <td>0.7066</td>\n",
       "      <td>0.699462</td>\n",
       "      <td>0.692208</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.076923</td>\n",
       "      <td>8.908360</td>\n",
       "      <td>29.6</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>30.143741</td>\n",
       "      <td>12.4</td>\n",
       "      <td>14.076923</td>\n",
       "      <td>11.303985</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>4.360070</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>1.538016</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.769231</td>\n",
       "      <td>10.604940</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.769231</td>\n",
       "      <td>15.611678</td>\n",
       "      <td>103.90</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>103.150813</td>\n",
       "      <td>121.06</td>\n",
       "      <td>109.784615</td>\n",
       "      <td>119.034924</td>\n",
       "      <td>68.42</td>\n",
       "      <td>70.946154</td>\n",
       "      <td>68.618108</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.277385</td>\n",
       "      <td>0.257188</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.358923</td>\n",
       "      <td>0.326383</td>\n",
       "      <td>0.5242</td>\n",
       "      <td>0.537846</td>\n",
       "      <td>0.520239</td>\n",
       "      <td>45.62</td>\n",
       "      <td>48.176923</td>\n",
       "      <td>47.158431</td>\n",
       "      <td>45.54</td>\n",
       "      <td>51.853846</td>\n",
       "      <td>42.403020</td>\n",
       "      <td>5.58</td>\n",
       "      <td>6.446154</td>\n",
       "      <td>6.386942</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.746154</td>\n",
       "      <td>4.025330</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.510077</td>\n",
       "      <td>0.493547</td>\n",
       "      <td>12.92</td>\n",
       "      <td>14.976923</td>\n",
       "      <td>13.566620</td>\n",
       "      <td>24.86</td>\n",
       "      <td>25.507692</td>\n",
       "      <td>27.214010</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>0.196385</td>\n",
       "      <td>0.181446</td>\n",
       "      <td>24.4</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.395855</td>\n",
       "      <td>63.6</td>\n",
       "      <td>58.928571</td>\n",
       "      <td>61.510901</td>\n",
       "      <td>0.3854</td>\n",
       "      <td>0.410214</td>\n",
       "      <td>0.398335</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.785714</td>\n",
       "      <td>7.628808</td>\n",
       "      <td>22.4</td>\n",
       "      <td>23.714286</td>\n",
       "      <td>23.477300</td>\n",
       "      <td>0.3066</td>\n",
       "      <td>0.338214</td>\n",
       "      <td>0.328224</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>6.257677</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.642857</td>\n",
       "      <td>8.428689</td>\n",
       "      <td>0.7242</td>\n",
       "      <td>0.700214</td>\n",
       "      <td>0.695112</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.857143</td>\n",
       "      <td>9.597987</td>\n",
       "      <td>32.2</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>29.879917</td>\n",
       "      <td>10.6</td>\n",
       "      <td>12.142857</td>\n",
       "      <td>12.808560</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.214286</td>\n",
       "      <td>7.066411</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.785714</td>\n",
       "      <td>3.534328</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.642857</td>\n",
       "      <td>12.163889</td>\n",
       "      <td>14.2</td>\n",
       "      <td>13.714286</td>\n",
       "      <td>13.984295</td>\n",
       "      <td>92.18</td>\n",
       "      <td>93.292857</td>\n",
       "      <td>92.539726</td>\n",
       "      <td>103.42</td>\n",
       "      <td>106.842857</td>\n",
       "      <td>109.305280</td>\n",
       "      <td>67.88</td>\n",
       "      <td>67.714286</td>\n",
       "      <td>67.448570</td>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.183643</td>\n",
       "      <td>0.137646</td>\n",
       "      <td>0.3524</td>\n",
       "      <td>0.404786</td>\n",
       "      <td>0.383154</td>\n",
       "      <td>0.4628</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.479864</td>\n",
       "      <td>47.60</td>\n",
       "      <td>46.442857</td>\n",
       "      <td>46.858256</td>\n",
       "      <td>43.14</td>\n",
       "      <td>50.171429</td>\n",
       "      <td>51.873574</td>\n",
       "      <td>10.30</td>\n",
       "      <td>10.628571</td>\n",
       "      <td>10.451685</td>\n",
       "      <td>8.86</td>\n",
       "      <td>11.350000</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.477643</td>\n",
       "      <td>0.460859</td>\n",
       "      <td>13.92</td>\n",
       "      <td>15.478571</td>\n",
       "      <td>15.672046</td>\n",
       "      <td>28.72</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>27.554073</td>\n",
       "      <td>0.1138</td>\n",
       "      <td>0.129357</td>\n",
       "      <td>0.101445</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FG_SMA     FG_CMA     FG_EMA  FGA_SMA    FGA_CMA    FGA_EMA  FG%_SMA  \\\n",
       "0    22.8  24.653846  23.211004     54.4  56.038462  55.951135   0.4192   \n",
       "1    26.0  25.363636  24.750136     58.6  57.000000  56.395143   0.4496   \n",
       "2    26.4  28.250000  25.671618     57.8  63.100000  56.427805   0.4536   \n",
       "3    24.6  25.666667  24.534714     52.2  54.000000  52.713938   0.4716   \n",
       "4    26.8  27.000000  26.409965     60.8  59.692308  60.456760   0.4400   \n",
       "\n",
       "    FG%_CMA   FG%_EMA  3P_SMA    3P_CMA    3P_EMA  3PA_SMA    3PA_CMA  \\\n",
       "0  0.440962  0.414875     5.0  6.307692  4.723021     17.4  20.076923   \n",
       "1  0.446727  0.442387     4.0  5.727273  4.457340     15.0  18.681818   \n",
       "2  0.445450  0.451120     6.4  6.650000  5.819351     20.6  20.400000   \n",
       "3  0.471333  0.464322     9.8  7.666667  9.373407     23.4  19.833333   \n",
       "4  0.447385  0.435041     7.4  7.615385  7.106061     19.8  21.307692   \n",
       "\n",
       "     3PA_EMA  3P%_SMA   3P%_CMA   3P%_EMA  FT_SMA     FT_CMA     FT_EMA  \\\n",
       "0  17.688046   0.2916  0.318923  0.265432    15.4  13.615385  12.571720   \n",
       "1  15.916814   0.2764  0.310500  0.282430    17.2  14.590909  17.632687   \n",
       "2  20.033660   0.3054  0.321850  0.288661     9.0   9.150000   9.090603   \n",
       "3  24.219688   0.4112  0.379583  0.380740    12.8  13.250000  13.450327   \n",
       "4  19.692654   0.3630  0.344846  0.351888    10.0  11.461538  10.878916   \n",
       "\n",
       "   FTA_SMA    FTA_CMA    FTA_EMA  FT%_SMA   FT%_CMA   FT%_EMA  ORB_SMA  \\\n",
       "0     20.6  18.807692  16.287396   0.7516  0.718615  0.756023      7.8   \n",
       "1     22.0  19.681818  21.997752   0.7602  0.735909  0.786252     11.4   \n",
       "2     14.2  13.950000  15.245384   0.5978  0.637750  0.577958      8.4   \n",
       "3     19.6  19.750000  20.433324   0.6826  0.681417  0.683046      7.0   \n",
       "4     14.0  16.230769  15.448029   0.7066  0.699462  0.692208      8.2   \n",
       "\n",
       "     ORB_CMA    ORB_EMA  TRB_SMA    TRB_CMA    TRB_EMA  AST_SMA    AST_CMA  \\\n",
       "0   7.423077   7.630525     30.6  29.538462  29.642729     12.0  12.307692   \n",
       "1   8.272727  10.136214     34.2  32.500000  32.159791     11.0  11.681818   \n",
       "2  10.150000   8.553029     28.0  32.750000  28.593819     15.6  16.050000   \n",
       "3   6.500000   7.127945     27.0  29.666667  29.147917     14.6  13.083333   \n",
       "4   8.076923   8.908360     29.6  30.769231  30.143741     12.4  14.076923   \n",
       "\n",
       "     AST_EMA  STL_SMA   STL_CMA   STL_EMA  BLK_SMA   BLK_CMA   BLK_EMA  \\\n",
       "0  11.694995      5.6  6.923077  6.909948      3.2  4.230769  3.675965   \n",
       "1  10.272923      5.8  5.500000  6.499974      3.0  2.909091  2.816150   \n",
       "2  14.707068      7.4  7.350000  7.326814      5.2  4.050000  5.228792   \n",
       "3  14.835013      6.0  5.583333  6.032555      1.4  2.416667  1.462046   \n",
       "4  11.303985      3.8  4.615385  4.360070      1.6  1.538462  1.538016   \n",
       "\n",
       "   TOV_SMA    TOV_CMA    TOV_EMA  PF_SMA     PF_CMA     PF_EMA  ORtg_SMA  \\\n",
       "0     12.0  11.384615  12.212031    20.2  16.769231  19.323787     96.86   \n",
       "1     10.6  10.863636  11.167679    16.4  15.454545  15.446714    108.50   \n",
       "2     13.0  12.850000  13.141423    14.2  14.550000  13.786065     97.50   \n",
       "3     13.8  14.583333  12.952757    19.0  19.583333  20.080306    105.38   \n",
       "4     10.0  11.769231  10.604940    15.0  16.769231  15.611678    103.90   \n",
       "\n",
       "     ORtg_CMA    ORtg_EMA  DRtg_SMA    DRtg_CMA    DRtg_EMA  Pace_SMA  \\\n",
       "0  100.507692   93.371986    104.72  101.753846  101.552341     68.06   \n",
       "1  103.040909  106.421923    105.94  101.304545  105.870044     67.44   \n",
       "2   99.430000   95.954924    107.38  102.715000  104.121329     69.56   \n",
       "3  100.925000  105.583149    101.46   99.975000   97.257504     68.54   \n",
       "4  102.500000  103.150813    121.06  109.784615  119.034924     68.42   \n",
       "\n",
       "    Pace_CMA   Pace_EMA  FTr_SMA   FTr_CMA   FTr_EMA  3PAr_SMA  3PAr_CMA  \\\n",
       "0  68.665385  67.742130   0.3852  0.342192  0.297102    0.3228  0.355769   \n",
       "1  68.240909  67.298025   0.3826  0.354000  0.396063    0.2590  0.328182   \n",
       "2  71.955000  68.716990   0.2554  0.227100  0.284286    0.3574  0.321400   \n",
       "3  70.741667  68.297726   0.3782  0.371417  0.389580    0.4490  0.370167   \n",
       "4  70.946154  68.618108   0.2294  0.277385  0.257188    0.3250  0.358923   \n",
       "\n",
       "   3PAr_EMA  TS%_SMA   TS%_CMA   TS%_EMA  TRB%_SMA   TRB%_CMA   TRB%_EMA  \\\n",
       "0  0.318668   0.5124  0.533500  0.498947     48.30  47.223077  47.416480   \n",
       "1  0.284080   0.5360  0.537273  0.539165     53.86  50.554545  51.135159   \n",
       "2  0.358103   0.5236  0.514950  0.516993     46.38  50.190000  46.864038   \n",
       "3  0.461446   0.5872  0.568833  0.578359     46.90  48.141667  49.164642   \n",
       "4  0.326383   0.5242  0.537846  0.520239     45.62  48.176923  47.158431   \n",
       "\n",
       "   AST%_SMA   AST%_CMA   AST%_EMA  STL%_SMA   STL%_CMA   STL%_EMA  BLK%_SMA  \\\n",
       "0     53.10  50.046154  50.683134      8.20  10.000000  10.137945      8.38   \n",
       "1     42.50  45.390909  41.533877      8.56   7.936364   9.626756     10.60   \n",
       "2     57.42  54.915000  56.400907     10.56  10.125000  10.660785     13.74   \n",
       "3     58.10  49.541667  59.472853      8.80   7.758333   8.803756      4.10   \n",
       "4     45.54  51.853846  42.403020      5.58   6.446154   6.386942      4.08   \n",
       "\n",
       "    BLK%_CMA   BLK%_EMA  eFG%_SMA  eFG%_CMA  eFG%_EMA  TOV%_SMA   TOV%_CMA  \\\n",
       "0  11.403846  10.344138    0.4658  0.497000  0.457629     15.76  14.819231   \n",
       "1   8.509091   8.985101    0.4846  0.497500  0.482619     13.44  14.113636   \n",
       "2  10.635000  14.307917    0.5080  0.497250  0.502925     16.86  15.710000   \n",
       "3   6.333333   4.202775    0.5662  0.542833  0.554087     18.10  18.691667   \n",
       "4   3.746154   4.025330    0.5006  0.510077  0.493547     12.92  14.976923   \n",
       "\n",
       "    TOV%_EMA  ORB%_SMA   ORB%_CMA   ORB%_EMA  FT/FGA_SMA  FT/FGA_CMA  \\\n",
       "0  16.077612     25.74  23.003846  24.421789      0.2866    0.249000   \n",
       "1  14.382758     34.70  25.840909  31.393061      0.3014    0.264045   \n",
       "2  17.228947     27.14  30.190000  27.231699      0.1584    0.147650   \n",
       "3  16.936403     23.86  22.291667  24.287561      0.2466    0.249083   \n",
       "4  13.566620     24.86  25.507692  27.214010      0.1640    0.196385   \n",
       "\n",
       "   FT/FGA_EMA  opp_FG_SMA  opp_FG_CMA  opp_FG_EMA  opp_FGA_SMA  opp_FGA_CMA  \\\n",
       "0    0.228649        21.4   21.538462   21.057987         54.8    52.538462   \n",
       "1    0.319346        27.8   26.869565   27.521328         60.0    58.217391   \n",
       "2    0.165701        26.0   25.454545   25.679242         57.4    55.090909   \n",
       "3    0.256014        26.2   25.928571   27.202928         56.6    58.857143   \n",
       "4    0.181446        24.4   24.000000   24.395855         63.6    58.928571   \n",
       "\n",
       "   opp_FGA_EMA  opp_FG%_SMA  opp_FG%_CMA  opp_FG%_EMA  opp_3P_SMA  opp_3P_CMA  \\\n",
       "0    54.855677       0.3910     0.409423     0.384831         8.2    8.076923   \n",
       "1    60.888683       0.4662     0.462348     0.454192         9.8    9.826087   \n",
       "2    57.564619       0.4572     0.464636     0.450118         8.2    8.545455   \n",
       "3    59.122634       0.4634     0.440357     0.460755         9.2    9.500000   \n",
       "4    61.510901       0.3854     0.410214     0.398335         6.8    7.785714   \n",
       "\n",
       "   opp_3P_EMA  opp_3PA_SMA  opp_3PA_CMA  opp_3PA_EMA  opp_3P%_SMA  \\\n",
       "0    8.884101         24.2    24.846154    25.275189       0.3360   \n",
       "1    7.595725         26.0    25.000000    23.311761       0.3486   \n",
       "2    7.799799         21.4    22.818182    21.633975       0.3872   \n",
       "3    9.989433         25.8    24.785714    27.224526       0.3538   \n",
       "4    7.628808         22.4    23.714286    23.477300       0.3066   \n",
       "\n",
       "   opp_3P%_CMA  opp_3P%_EMA  opp_FT_SMA  opp_FT_CMA  opp_FT_EMA  opp_FTA_SMA  \\\n",
       "0     0.321192     0.350582        10.4   11.846154   11.148380         14.6   \n",
       "1     0.383913     0.288509         8.2   10.521739    7.798636         11.2   \n",
       "2     0.377591     0.365943        13.2   11.454545   11.706824         18.6   \n",
       "3     0.379286     0.365979        10.8   11.071429   10.378299         14.6   \n",
       "4     0.338214     0.328224         7.2    7.571429    6.257677          9.4   \n",
       "\n",
       "   opp_FTA_CMA  opp_FTA_EMA  opp_FT%_SMA  opp_FT%_CMA  opp_FT%_EMA  \\\n",
       "0    15.884615    14.613588       0.7082     0.739269     0.753357   \n",
       "1    14.869565    11.644625       0.7730     0.727348     0.720520   \n",
       "2    17.318182    17.203915       0.7188     0.644227     0.701938   \n",
       "3    14.857143    13.676782       0.7270     0.746500     0.767052   \n",
       "4    10.642857     8.428689       0.7242     0.700214     0.695112   \n",
       "\n",
       "   opp_ORB_SMA  opp_ORB_CMA  opp_ORB_EMA  opp_TRB_SMA  opp_TRB_CMA  \\\n",
       "0          6.8     6.115385     7.383697         31.4    30.653846   \n",
       "1          7.8     6.826087     8.042637         32.2    29.043478   \n",
       "2          8.8     9.272727     8.568962         34.8    34.318182   \n",
       "3          6.2     6.857143     6.908517         27.0    29.857143   \n",
       "4         10.6     7.857143     9.597987         32.2    28.571429   \n",
       "\n",
       "   opp_TRB_EMA  opp_AST_SMA  opp_AST_CMA  opp_AST_EMA  opp_STL_SMA  \\\n",
       "0    32.536725         11.8    11.653846    11.233467          3.4   \n",
       "1    32.869443         14.8    15.260870    13.426693          5.0   \n",
       "2    34.254804         15.8    16.181818    14.151697          5.0   \n",
       "3    28.080932         17.2    15.714286    18.015524          8.8   \n",
       "4    29.879917         10.6    12.142857    12.808560          7.0   \n",
       "\n",
       "   opp_STL_CMA  opp_STL_EMA  opp_BLK_SMA  opp_BLK_CMA  opp_BLK_EMA  \\\n",
       "0     4.192308     3.474109          2.8     2.346154     2.342764   \n",
       "1     7.260870     5.431505          3.6     3.478261     3.540045   \n",
       "2     5.954545     4.269750          5.8     5.772727     5.840287   \n",
       "3     6.857143     8.948778          5.4     4.357143     5.081276   \n",
       "4     7.214286     7.066411          3.0     3.785714     3.534328   \n",
       "\n",
       "   opp_TOV_SMA  opp_TOV_CMA  opp_TOV_EMA  opp_PF_SMA  opp_PF_CMA  opp_PF_EMA  \\\n",
       "0         11.6    11.346154    13.049714        15.0   14.653846   15.671923   \n",
       "1         10.0    11.391304    10.488640        15.6   16.000000   14.994876   \n",
       "2         11.2    13.681818    12.204707        17.8   14.727273   18.470674   \n",
       "3         18.6    17.500000    18.342378        19.4   17.357143   18.155262   \n",
       "4         11.0    11.642857    12.163889        14.2   13.714286   13.984295   \n",
       "\n",
       "   opp_ORtg_SMA  opp_ORtg_CMA  opp_ORtg_EMA  opp_DRtg_SMA  opp_DRtg_CMA  \\\n",
       "0         92.98     96.769231     93.931371        102.02    104.400000   \n",
       "1        108.82    106.200000    102.699579        105.02    100.552174   \n",
       "2        105.80    104.645455    102.108583        100.98     95.209091   \n",
       "3         96.16     94.407143     97.725044        103.50    108.471429   \n",
       "4         92.18     93.292857     92.539726        103.42    106.842857   \n",
       "\n",
       "   opp_DRtg_EMA  opp_Pace_SMA  opp_Pace_CMA  opp_Pace_EMA  opp_FTr_SMA  \\\n",
       "0     99.376988         65.82     65.400000     66.126001       0.2672   \n",
       "1    103.770446         67.90     69.226087     68.827918       0.1854   \n",
       "2    100.397883         67.80     67.559091     67.625642       0.3282   \n",
       "3    105.612118         75.26     75.585714     76.090106       0.2588   \n",
       "4    109.305280         67.88     67.714286     67.448570       0.1486   \n",
       "\n",
       "   opp_FTr_CMA  opp_FTr_EMA  opp_3PAr_SMA  opp_3PAr_CMA  opp_3PAr_EMA  \\\n",
       "0     0.316538     0.267147        0.4408      0.468308      0.459948   \n",
       "1     0.254087     0.189748        0.4412      0.430783      0.389290   \n",
       "2     0.329273     0.302344        0.3732      0.417636      0.375464   \n",
       "3     0.249857     0.229168        0.4548      0.419500      0.460251   \n",
       "4     0.183643     0.137646        0.3524      0.404786      0.383154   \n",
       "\n",
       "   opp_TS%_SMA  opp_TS%_CMA  opp_TS%_EMA  opp_TRB%_SMA  opp_TRB%_CMA  \\\n",
       "0       0.4962     0.523346     0.502494         47.46     49.119231   \n",
       "1       0.5694     0.568565     0.535933         50.22     47.143478   \n",
       "2       0.5586     0.563136     0.543310         51.72     54.690909   \n",
       "3       0.5706     0.547571     0.569564         45.38     46.114286   \n",
       "4       0.4628     0.498000     0.479864         47.60     46.442857   \n",
       "\n",
       "   opp_TRB%_EMA  opp_AST%_SMA  opp_AST%_CMA  opp_AST%_EMA  opp_STL%_SMA  \\\n",
       "0     49.264062         55.60     53.553846     53.411235          5.08   \n",
       "1     51.452466         52.96     57.156522     48.568758          7.34   \n",
       "2     51.240622         60.86     63.813636     55.153792          7.18   \n",
       "3     46.860574         64.50     59.807143     65.736169         11.76   \n",
       "4     46.858256         43.14     50.171429     51.873574         10.30   \n",
       "\n",
       "   opp_STL%_CMA  opp_STL%_EMA  opp_BLK%_SMA  opp_BLK%_CMA  opp_BLK%_EMA  \\\n",
       "0      6.373077      5.167442          6.86      7.053846      6.029065   \n",
       "1     10.417391      7.870382          7.20      9.239130      7.712390   \n",
       "2      8.763636      6.169933         14.94     14.968182     14.708419   \n",
       "3      9.042857     11.850792         13.44     10.992857     12.783031   \n",
       "4     10.628571     10.451685          8.86     11.350000     10.819778   \n",
       "\n",
       "   opp_eFG%_SMA  opp_eFG%_CMA  opp_eFG%_EMA  opp_TOV%_SMA  opp_TOV%_CMA  \\\n",
       "0        0.4656      0.484962      0.465742         15.52     15.811538   \n",
       "1        0.5504      0.547261      0.518834         13.40     14.778261   \n",
       "2        0.5294      0.544273      0.518760         14.50     17.772727   \n",
       "3        0.5440      0.520357      0.544734         22.70     21.035714   \n",
       "4        0.4392      0.477643      0.460859         13.92     15.478571   \n",
       "\n",
       "   opp_TOV%_EMA  opp_ORB%_SMA  opp_ORB%_CMA  opp_ORB%_EMA  opp_FT/FGA_SMA  \\\n",
       "0     17.140137         22.40     20.638462     24.324966          0.1900   \n",
       "1     13.688026         24.02     22.078261     24.345978          0.1374   \n",
       "2     15.755138         26.50     30.259091     25.897915          0.2354   \n",
       "3     21.910853         21.30     21.192857     23.197858          0.1908   \n",
       "4     15.672046         28.72     23.400000     27.554073          0.1138   \n",
       "\n",
       "   opp_FT/FGA_CMA  opp_FT/FGA_EMA  Neutral  Win  Loss  \n",
       "0        0.236269        0.203534        0    1     0  \n",
       "1        0.180522        0.128987        0    1     0  \n",
       "2        0.220864        0.207807        0    0     1  \n",
       "3        0.185571        0.173572        0    0     1  \n",
       "4        0.129357        0.101445        0    1     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.abspath(f'../../data/training_set.csv')\n",
    "training_df = pd.read_csv(path)\n",
    "training_df['Loss'] = 1 - training_df['Win']\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG_SMA</th>\n",
       "      <th>FG_CMA</th>\n",
       "      <th>FG_EMA</th>\n",
       "      <th>FGA_SMA</th>\n",
       "      <th>FGA_CMA</th>\n",
       "      <th>FGA_EMA</th>\n",
       "      <th>FG%_SMA</th>\n",
       "      <th>FG%_CMA</th>\n",
       "      <th>FG%_EMA</th>\n",
       "      <th>3P_SMA</th>\n",
       "      <th>3P_CMA</th>\n",
       "      <th>3P_EMA</th>\n",
       "      <th>3PA_SMA</th>\n",
       "      <th>3PA_CMA</th>\n",
       "      <th>3PA_EMA</th>\n",
       "      <th>3P%_SMA</th>\n",
       "      <th>3P%_CMA</th>\n",
       "      <th>3P%_EMA</th>\n",
       "      <th>FT_SMA</th>\n",
       "      <th>FT_CMA</th>\n",
       "      <th>FT_EMA</th>\n",
       "      <th>FTA_SMA</th>\n",
       "      <th>FTA_CMA</th>\n",
       "      <th>FTA_EMA</th>\n",
       "      <th>FT%_SMA</th>\n",
       "      <th>FT%_CMA</th>\n",
       "      <th>FT%_EMA</th>\n",
       "      <th>ORB_SMA</th>\n",
       "      <th>ORB_CMA</th>\n",
       "      <th>ORB_EMA</th>\n",
       "      <th>TRB_SMA</th>\n",
       "      <th>TRB_CMA</th>\n",
       "      <th>TRB_EMA</th>\n",
       "      <th>AST_SMA</th>\n",
       "      <th>AST_CMA</th>\n",
       "      <th>AST_EMA</th>\n",
       "      <th>STL_SMA</th>\n",
       "      <th>STL_CMA</th>\n",
       "      <th>STL_EMA</th>\n",
       "      <th>BLK_SMA</th>\n",
       "      <th>BLK_CMA</th>\n",
       "      <th>BLK_EMA</th>\n",
       "      <th>TOV_SMA</th>\n",
       "      <th>TOV_CMA</th>\n",
       "      <th>TOV_EMA</th>\n",
       "      <th>PF_SMA</th>\n",
       "      <th>PF_CMA</th>\n",
       "      <th>PF_EMA</th>\n",
       "      <th>ORtg_SMA</th>\n",
       "      <th>ORtg_CMA</th>\n",
       "      <th>ORtg_EMA</th>\n",
       "      <th>DRtg_SMA</th>\n",
       "      <th>DRtg_CMA</th>\n",
       "      <th>DRtg_EMA</th>\n",
       "      <th>Pace_SMA</th>\n",
       "      <th>Pace_CMA</th>\n",
       "      <th>Pace_EMA</th>\n",
       "      <th>FTr_SMA</th>\n",
       "      <th>FTr_CMA</th>\n",
       "      <th>FTr_EMA</th>\n",
       "      <th>3PAr_SMA</th>\n",
       "      <th>3PAr_CMA</th>\n",
       "      <th>3PAr_EMA</th>\n",
       "      <th>TS%_SMA</th>\n",
       "      <th>TS%_CMA</th>\n",
       "      <th>TS%_EMA</th>\n",
       "      <th>TRB%_SMA</th>\n",
       "      <th>TRB%_CMA</th>\n",
       "      <th>TRB%_EMA</th>\n",
       "      <th>AST%_SMA</th>\n",
       "      <th>AST%_CMA</th>\n",
       "      <th>AST%_EMA</th>\n",
       "      <th>STL%_SMA</th>\n",
       "      <th>STL%_CMA</th>\n",
       "      <th>STL%_EMA</th>\n",
       "      <th>BLK%_SMA</th>\n",
       "      <th>BLK%_CMA</th>\n",
       "      <th>BLK%_EMA</th>\n",
       "      <th>eFG%_SMA</th>\n",
       "      <th>eFG%_CMA</th>\n",
       "      <th>eFG%_EMA</th>\n",
       "      <th>TOV%_SMA</th>\n",
       "      <th>TOV%_CMA</th>\n",
       "      <th>TOV%_EMA</th>\n",
       "      <th>ORB%_SMA</th>\n",
       "      <th>ORB%_CMA</th>\n",
       "      <th>ORB%_EMA</th>\n",
       "      <th>FT/FGA_SMA</th>\n",
       "      <th>FT/FGA_CMA</th>\n",
       "      <th>FT/FGA_EMA</th>\n",
       "      <th>opp_FG_SMA</th>\n",
       "      <th>opp_FG_CMA</th>\n",
       "      <th>opp_FG_EMA</th>\n",
       "      <th>opp_FGA_SMA</th>\n",
       "      <th>opp_FGA_CMA</th>\n",
       "      <th>opp_FGA_EMA</th>\n",
       "      <th>opp_FG%_SMA</th>\n",
       "      <th>opp_FG%_CMA</th>\n",
       "      <th>opp_FG%_EMA</th>\n",
       "      <th>opp_3P_SMA</th>\n",
       "      <th>opp_3P_CMA</th>\n",
       "      <th>opp_3P_EMA</th>\n",
       "      <th>opp_3PA_SMA</th>\n",
       "      <th>opp_3PA_CMA</th>\n",
       "      <th>opp_3PA_EMA</th>\n",
       "      <th>opp_3P%_SMA</th>\n",
       "      <th>opp_3P%_CMA</th>\n",
       "      <th>opp_3P%_EMA</th>\n",
       "      <th>opp_FT_SMA</th>\n",
       "      <th>opp_FT_CMA</th>\n",
       "      <th>opp_FT_EMA</th>\n",
       "      <th>opp_FTA_SMA</th>\n",
       "      <th>opp_FTA_CMA</th>\n",
       "      <th>opp_FTA_EMA</th>\n",
       "      <th>opp_FT%_SMA</th>\n",
       "      <th>opp_FT%_CMA</th>\n",
       "      <th>opp_FT%_EMA</th>\n",
       "      <th>opp_ORB_SMA</th>\n",
       "      <th>opp_ORB_CMA</th>\n",
       "      <th>opp_ORB_EMA</th>\n",
       "      <th>opp_TRB_SMA</th>\n",
       "      <th>opp_TRB_CMA</th>\n",
       "      <th>opp_TRB_EMA</th>\n",
       "      <th>opp_AST_SMA</th>\n",
       "      <th>opp_AST_CMA</th>\n",
       "      <th>opp_AST_EMA</th>\n",
       "      <th>opp_STL_SMA</th>\n",
       "      <th>opp_STL_CMA</th>\n",
       "      <th>opp_STL_EMA</th>\n",
       "      <th>opp_BLK_SMA</th>\n",
       "      <th>opp_BLK_CMA</th>\n",
       "      <th>opp_BLK_EMA</th>\n",
       "      <th>opp_TOV_SMA</th>\n",
       "      <th>opp_TOV_CMA</th>\n",
       "      <th>opp_TOV_EMA</th>\n",
       "      <th>opp_PF_SMA</th>\n",
       "      <th>opp_PF_CMA</th>\n",
       "      <th>opp_PF_EMA</th>\n",
       "      <th>opp_ORtg_SMA</th>\n",
       "      <th>opp_ORtg_CMA</th>\n",
       "      <th>opp_ORtg_EMA</th>\n",
       "      <th>opp_DRtg_SMA</th>\n",
       "      <th>opp_DRtg_CMA</th>\n",
       "      <th>opp_DRtg_EMA</th>\n",
       "      <th>opp_Pace_SMA</th>\n",
       "      <th>opp_Pace_CMA</th>\n",
       "      <th>opp_Pace_EMA</th>\n",
       "      <th>opp_FTr_SMA</th>\n",
       "      <th>opp_FTr_CMA</th>\n",
       "      <th>opp_FTr_EMA</th>\n",
       "      <th>opp_3PAr_SMA</th>\n",
       "      <th>opp_3PAr_CMA</th>\n",
       "      <th>opp_3PAr_EMA</th>\n",
       "      <th>opp_TS%_SMA</th>\n",
       "      <th>opp_TS%_CMA</th>\n",
       "      <th>opp_TS%_EMA</th>\n",
       "      <th>opp_TRB%_SMA</th>\n",
       "      <th>opp_TRB%_CMA</th>\n",
       "      <th>opp_TRB%_EMA</th>\n",
       "      <th>opp_AST%_SMA</th>\n",
       "      <th>opp_AST%_CMA</th>\n",
       "      <th>opp_AST%_EMA</th>\n",
       "      <th>opp_STL%_SMA</th>\n",
       "      <th>opp_STL%_CMA</th>\n",
       "      <th>opp_STL%_EMA</th>\n",
       "      <th>opp_BLK%_SMA</th>\n",
       "      <th>opp_BLK%_CMA</th>\n",
       "      <th>opp_BLK%_EMA</th>\n",
       "      <th>opp_eFG%_SMA</th>\n",
       "      <th>opp_eFG%_CMA</th>\n",
       "      <th>opp_eFG%_EMA</th>\n",
       "      <th>opp_TOV%_SMA</th>\n",
       "      <th>opp_TOV%_CMA</th>\n",
       "      <th>opp_TOV%_EMA</th>\n",
       "      <th>opp_ORB%_SMA</th>\n",
       "      <th>opp_ORB%_CMA</th>\n",
       "      <th>opp_ORB%_EMA</th>\n",
       "      <th>opp_FT/FGA_SMA</th>\n",
       "      <th>opp_FT/FGA_CMA</th>\n",
       "      <th>opp_FT/FGA_EMA</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Win</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.6</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>26.295223</td>\n",
       "      <td>57.2</td>\n",
       "      <td>58.588235</td>\n",
       "      <td>59.950459</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.427588</td>\n",
       "      <td>0.434373</td>\n",
       "      <td>7.4</td>\n",
       "      <td>6.588235</td>\n",
       "      <td>8.204161</td>\n",
       "      <td>23.6</td>\n",
       "      <td>22.176471</td>\n",
       "      <td>24.070312</td>\n",
       "      <td>0.3084</td>\n",
       "      <td>0.299059</td>\n",
       "      <td>0.337075</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.647059</td>\n",
       "      <td>13.172682</td>\n",
       "      <td>20.8</td>\n",
       "      <td>19.235294</td>\n",
       "      <td>19.369426</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>0.650882</td>\n",
       "      <td>0.685984</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.705882</td>\n",
       "      <td>8.663763</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>33.494042</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.058824</td>\n",
       "      <td>16.393277</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.529412</td>\n",
       "      <td>7.253464</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.529412</td>\n",
       "      <td>3.867263</td>\n",
       "      <td>12.8</td>\n",
       "      <td>13.588235</td>\n",
       "      <td>12.652723</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.529412</td>\n",
       "      <td>18.925827</td>\n",
       "      <td>99.60</td>\n",
       "      <td>96.376471</td>\n",
       "      <td>102.177531</td>\n",
       "      <td>103.68</td>\n",
       "      <td>99.011765</td>\n",
       "      <td>100.365633</td>\n",
       "      <td>71.06</td>\n",
       "      <td>72.029412</td>\n",
       "      <td>72.268344</td>\n",
       "      <td>0.3742</td>\n",
       "      <td>0.336706</td>\n",
       "      <td>0.333050</td>\n",
       "      <td>0.4134</td>\n",
       "      <td>0.378235</td>\n",
       "      <td>0.402471</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.512118</td>\n",
       "      <td>0.532588</td>\n",
       "      <td>51.98</td>\n",
       "      <td>51.194118</td>\n",
       "      <td>53.618324</td>\n",
       "      <td>61.28</td>\n",
       "      <td>59.847059</td>\n",
       "      <td>61.491816</td>\n",
       "      <td>8.30</td>\n",
       "      <td>8.976471</td>\n",
       "      <td>9.928151</td>\n",
       "      <td>12.04</td>\n",
       "      <td>10.111765</td>\n",
       "      <td>12.098830</td>\n",
       "      <td>0.4896</td>\n",
       "      <td>0.483588</td>\n",
       "      <td>0.501728</td>\n",
       "      <td>16.10</td>\n",
       "      <td>16.652941</td>\n",
       "      <td>15.522065</td>\n",
       "      <td>25.48</td>\n",
       "      <td>26.558824</td>\n",
       "      <td>27.283188</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>0.221882</td>\n",
       "      <td>0.224880</td>\n",
       "      <td>25.6</td>\n",
       "      <td>26.263158</td>\n",
       "      <td>24.161796</td>\n",
       "      <td>54.6</td>\n",
       "      <td>59.578947</td>\n",
       "      <td>54.535112</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.440586</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.368421</td>\n",
       "      <td>6.938694</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.789474</td>\n",
       "      <td>19.170009</td>\n",
       "      <td>0.3640</td>\n",
       "      <td>0.353421</td>\n",
       "      <td>0.346890</td>\n",
       "      <td>13.8</td>\n",
       "      <td>12.157895</td>\n",
       "      <td>14.134105</td>\n",
       "      <td>21.6</td>\n",
       "      <td>19.684211</td>\n",
       "      <td>23.175522</td>\n",
       "      <td>0.6444</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.617173</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.947368</td>\n",
       "      <td>10.232045</td>\n",
       "      <td>34.4</td>\n",
       "      <td>36.736842</td>\n",
       "      <td>37.302457</td>\n",
       "      <td>16.4</td>\n",
       "      <td>16.052632</td>\n",
       "      <td>15.467661</td>\n",
       "      <td>11.8</td>\n",
       "      <td>9.263158</td>\n",
       "      <td>10.163377</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>5.158201</td>\n",
       "      <td>19.2</td>\n",
       "      <td>16.263158</td>\n",
       "      <td>19.961327</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.315789</td>\n",
       "      <td>19.325415</td>\n",
       "      <td>97.32</td>\n",
       "      <td>97.121053</td>\n",
       "      <td>92.661265</td>\n",
       "      <td>81.94</td>\n",
       "      <td>84.326316</td>\n",
       "      <td>76.493108</td>\n",
       "      <td>74.54</td>\n",
       "      <td>74.047368</td>\n",
       "      <td>75.133121</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.429435</td>\n",
       "      <td>0.3626</td>\n",
       "      <td>0.350158</td>\n",
       "      <td>0.351612</td>\n",
       "      <td>0.5588</td>\n",
       "      <td>0.523474</td>\n",
       "      <td>0.528957</td>\n",
       "      <td>56.72</td>\n",
       "      <td>55.089474</td>\n",
       "      <td>58.697837</td>\n",
       "      <td>65.74</td>\n",
       "      <td>61.347368</td>\n",
       "      <td>65.274972</td>\n",
       "      <td>15.62</td>\n",
       "      <td>12.384211</td>\n",
       "      <td>13.507466</td>\n",
       "      <td>12.94</td>\n",
       "      <td>15.321053</td>\n",
       "      <td>14.292547</td>\n",
       "      <td>0.5340</td>\n",
       "      <td>0.503316</td>\n",
       "      <td>0.503701</td>\n",
       "      <td>22.70</td>\n",
       "      <td>19.210526</td>\n",
       "      <td>23.150352</td>\n",
       "      <td>32.98</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>33.972093</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.206421</td>\n",
       "      <td>0.261465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.4</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>26.894015</td>\n",
       "      <td>54.8</td>\n",
       "      <td>55.916667</td>\n",
       "      <td>55.720876</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>0.454417</td>\n",
       "      <td>0.480009</td>\n",
       "      <td>7.4</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>7.206811</td>\n",
       "      <td>22.2</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.849769</td>\n",
       "      <td>0.3374</td>\n",
       "      <td>0.326417</td>\n",
       "      <td>0.345893</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>15.147651</td>\n",
       "      <td>17.4</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.813979</td>\n",
       "      <td>0.8158</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>0.804623</td>\n",
       "      <td>5.4</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>5.983827</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.083333</td>\n",
       "      <td>31.540353</td>\n",
       "      <td>13.6</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>14.659644</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>7.526371</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>5.913450</td>\n",
       "      <td>12.8</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.254156</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.416667</td>\n",
       "      <td>14.752449</td>\n",
       "      <td>104.66</td>\n",
       "      <td>101.841667</td>\n",
       "      <td>108.833516</td>\n",
       "      <td>95.44</td>\n",
       "      <td>92.383333</td>\n",
       "      <td>91.727737</td>\n",
       "      <td>69.40</td>\n",
       "      <td>69.533333</td>\n",
       "      <td>69.843951</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>0.349167</td>\n",
       "      <td>0.350921</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>0.344750</td>\n",
       "      <td>0.374650</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>0.552167</td>\n",
       "      <td>0.587170</td>\n",
       "      <td>48.32</td>\n",
       "      <td>49.283333</td>\n",
       "      <td>50.794579</td>\n",
       "      <td>52.06</td>\n",
       "      <td>43.458333</td>\n",
       "      <td>51.963275</td>\n",
       "      <td>11.52</td>\n",
       "      <td>10.166667</td>\n",
       "      <td>10.706601</td>\n",
       "      <td>15.08</td>\n",
       "      <td>17.758333</td>\n",
       "      <td>15.824199</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.511917</td>\n",
       "      <td>0.544585</td>\n",
       "      <td>16.82</td>\n",
       "      <td>16.691667</td>\n",
       "      <td>15.854449</td>\n",
       "      <td>20.80</td>\n",
       "      <td>24.325000</td>\n",
       "      <td>22.854488</td>\n",
       "      <td>0.2676</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.282660</td>\n",
       "      <td>26.4</td>\n",
       "      <td>25.454545</td>\n",
       "      <td>24.685058</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.363636</td>\n",
       "      <td>53.919338</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.459139</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>6.227337</td>\n",
       "      <td>19.4</td>\n",
       "      <td>21.363636</td>\n",
       "      <td>20.010720</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>0.291818</td>\n",
       "      <td>0.304787</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.636364</td>\n",
       "      <td>17.081051</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.181818</td>\n",
       "      <td>24.057105</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>0.675364</td>\n",
       "      <td>0.709332</td>\n",
       "      <td>9.2</td>\n",
       "      <td>11.181818</td>\n",
       "      <td>9.276042</td>\n",
       "      <td>32.6</td>\n",
       "      <td>35.636364</td>\n",
       "      <td>31.523480</td>\n",
       "      <td>12.4</td>\n",
       "      <td>13.454545</td>\n",
       "      <td>13.094904</td>\n",
       "      <td>7.6</td>\n",
       "      <td>8.727273</td>\n",
       "      <td>7.716964</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.545455</td>\n",
       "      <td>6.531423</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.454545</td>\n",
       "      <td>15.302749</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.272727</td>\n",
       "      <td>20.626886</td>\n",
       "      <td>104.10</td>\n",
       "      <td>102.254545</td>\n",
       "      <td>102.612163</td>\n",
       "      <td>95.84</td>\n",
       "      <td>87.200000</td>\n",
       "      <td>96.516332</td>\n",
       "      <td>72.10</td>\n",
       "      <td>71.272727</td>\n",
       "      <td>70.806744</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.408545</td>\n",
       "      <td>0.460333</td>\n",
       "      <td>0.3428</td>\n",
       "      <td>0.370273</td>\n",
       "      <td>0.375382</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.558821</td>\n",
       "      <td>52.18</td>\n",
       "      <td>53.881818</td>\n",
       "      <td>53.677695</td>\n",
       "      <td>48.30</td>\n",
       "      <td>53.681818</td>\n",
       "      <td>54.299240</td>\n",
       "      <td>10.64</td>\n",
       "      <td>12.181818</td>\n",
       "      <td>10.918891</td>\n",
       "      <td>17.34</td>\n",
       "      <td>18.818182</td>\n",
       "      <td>16.227338</td>\n",
       "      <td>0.5164</td>\n",
       "      <td>0.489273</td>\n",
       "      <td>0.517752</td>\n",
       "      <td>16.48</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>19.017409</td>\n",
       "      <td>32.02</td>\n",
       "      <td>35.209091</td>\n",
       "      <td>35.178476</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.277182</td>\n",
       "      <td>0.328098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.6</td>\n",
       "      <td>23.625000</td>\n",
       "      <td>23.893919</td>\n",
       "      <td>57.4</td>\n",
       "      <td>57.750000</td>\n",
       "      <td>57.110197</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.418203</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.976223</td>\n",
       "      <td>23.8</td>\n",
       "      <td>22.375000</td>\n",
       "      <td>22.977595</td>\n",
       "      <td>0.3246</td>\n",
       "      <td>0.315375</td>\n",
       "      <td>0.355482</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.125000</td>\n",
       "      <td>13.397805</td>\n",
       "      <td>18.8</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>17.759031</td>\n",
       "      <td>0.7792</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.743282</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>10.604938</td>\n",
       "      <td>36.2</td>\n",
       "      <td>36.125000</td>\n",
       "      <td>36.867398</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.875000</td>\n",
       "      <td>13.618656</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7.375000</td>\n",
       "      <td>6.412894</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>3.427526</td>\n",
       "      <td>13.6</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.100594</td>\n",
       "      <td>13.8</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.414723</td>\n",
       "      <td>98.60</td>\n",
       "      <td>98.512500</td>\n",
       "      <td>100.778692</td>\n",
       "      <td>87.28</td>\n",
       "      <td>89.612500</td>\n",
       "      <td>85.721445</td>\n",
       "      <td>68.42</td>\n",
       "      <td>70.312500</td>\n",
       "      <td>68.383082</td>\n",
       "      <td>0.3328</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.314520</td>\n",
       "      <td>0.4158</td>\n",
       "      <td>0.386625</td>\n",
       "      <td>0.402249</td>\n",
       "      <td>0.5122</td>\n",
       "      <td>0.517625</td>\n",
       "      <td>0.528738</td>\n",
       "      <td>54.60</td>\n",
       "      <td>55.950000</td>\n",
       "      <td>55.052309</td>\n",
       "      <td>55.24</td>\n",
       "      <td>51.325000</td>\n",
       "      <td>57.850754</td>\n",
       "      <td>9.68</td>\n",
       "      <td>10.475000</td>\n",
       "      <td>9.387151</td>\n",
       "      <td>8.18</td>\n",
       "      <td>8.675000</td>\n",
       "      <td>7.962323</td>\n",
       "      <td>0.4598</td>\n",
       "      <td>0.469750</td>\n",
       "      <td>0.489303</td>\n",
       "      <td>17.04</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>17.707910</td>\n",
       "      <td>33.68</td>\n",
       "      <td>34.762500</td>\n",
       "      <td>33.959442</td>\n",
       "      <td>0.2662</td>\n",
       "      <td>0.265250</td>\n",
       "      <td>0.237552</td>\n",
       "      <td>20.6</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>21.045267</td>\n",
       "      <td>57.0</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>57.646091</td>\n",
       "      <td>0.3620</td>\n",
       "      <td>0.364167</td>\n",
       "      <td>0.366605</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.946502</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>15.786008</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>0.312971</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.166667</td>\n",
       "      <td>11.950617</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>16.748971</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.686333</td>\n",
       "      <td>0.719436</td>\n",
       "      <td>9.2</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>9.197531</td>\n",
       "      <td>29.8</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>28.954733</td>\n",
       "      <td>9.6</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>9.444444</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>7.148148</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.415638</td>\n",
       "      <td>17.8</td>\n",
       "      <td>17.833333</td>\n",
       "      <td>17.629630</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.833333</td>\n",
       "      <td>16.588477</td>\n",
       "      <td>78.24</td>\n",
       "      <td>76.233333</td>\n",
       "      <td>79.395473</td>\n",
       "      <td>110.28</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>114.295062</td>\n",
       "      <td>72.06</td>\n",
       "      <td>71.316667</td>\n",
       "      <td>71.867901</td>\n",
       "      <td>0.3118</td>\n",
       "      <td>0.287667</td>\n",
       "      <td>0.282848</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.278337</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.452086</td>\n",
       "      <td>46.94</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>45.559671</td>\n",
       "      <td>45.88</td>\n",
       "      <td>42.866667</td>\n",
       "      <td>44.186831</td>\n",
       "      <td>9.24</td>\n",
       "      <td>8.933333</td>\n",
       "      <td>9.470782</td>\n",
       "      <td>8.90</td>\n",
       "      <td>7.416667</td>\n",
       "      <td>7.004527</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>0.403333</td>\n",
       "      <td>0.409926</td>\n",
       "      <td>21.58</td>\n",
       "      <td>22.283333</td>\n",
       "      <td>21.498765</td>\n",
       "      <td>26.84</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>26.860494</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.196667</td>\n",
       "      <td>0.202350</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.6</td>\n",
       "      <td>30.210526</td>\n",
       "      <td>27.649443</td>\n",
       "      <td>59.4</td>\n",
       "      <td>61.684211</td>\n",
       "      <td>62.042109</td>\n",
       "      <td>0.4672</td>\n",
       "      <td>0.487421</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>9.2</td>\n",
       "      <td>8.842105</td>\n",
       "      <td>8.484374</td>\n",
       "      <td>26.6</td>\n",
       "      <td>23.736842</td>\n",
       "      <td>27.077445</td>\n",
       "      <td>0.3448</td>\n",
       "      <td>0.362474</td>\n",
       "      <td>0.312032</td>\n",
       "      <td>12.8</td>\n",
       "      <td>13.368421</td>\n",
       "      <td>15.358436</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.947368</td>\n",
       "      <td>21.963728</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>0.661421</td>\n",
       "      <td>0.680866</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.315789</td>\n",
       "      <td>13.258659</td>\n",
       "      <td>39.2</td>\n",
       "      <td>37.263158</td>\n",
       "      <td>40.132135</td>\n",
       "      <td>16.8</td>\n",
       "      <td>15.894737</td>\n",
       "      <td>16.683856</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.105263</td>\n",
       "      <td>8.039175</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.421053</td>\n",
       "      <td>4.673308</td>\n",
       "      <td>10.2</td>\n",
       "      <td>11.631579</td>\n",
       "      <td>9.552080</td>\n",
       "      <td>17.2</td>\n",
       "      <td>15.473684</td>\n",
       "      <td>16.718513</td>\n",
       "      <td>115.44</td>\n",
       "      <td>115.610526</td>\n",
       "      <td>115.290975</td>\n",
       "      <td>79.34</td>\n",
       "      <td>88.810526</td>\n",
       "      <td>84.296366</td>\n",
       "      <td>67.32</td>\n",
       "      <td>71.278947</td>\n",
       "      <td>69.146828</td>\n",
       "      <td>0.3318</td>\n",
       "      <td>0.328737</td>\n",
       "      <td>0.355954</td>\n",
       "      <td>0.4474</td>\n",
       "      <td>0.386737</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.578579</td>\n",
       "      <td>0.550669</td>\n",
       "      <td>59.50</td>\n",
       "      <td>56.963158</td>\n",
       "      <td>58.839409</td>\n",
       "      <td>59.84</td>\n",
       "      <td>52.747368</td>\n",
       "      <td>59.355313</td>\n",
       "      <td>11.86</td>\n",
       "      <td>11.331579</td>\n",
       "      <td>11.589912</td>\n",
       "      <td>14.14</td>\n",
       "      <td>10.215789</td>\n",
       "      <td>14.153236</td>\n",
       "      <td>0.5466</td>\n",
       "      <td>0.559684</td>\n",
       "      <td>0.516950</td>\n",
       "      <td>13.04</td>\n",
       "      <td>14.105263</td>\n",
       "      <td>11.757133</td>\n",
       "      <td>38.50</td>\n",
       "      <td>36.057895</td>\n",
       "      <td>39.413917</td>\n",
       "      <td>0.2168</td>\n",
       "      <td>0.218789</td>\n",
       "      <td>0.247359</td>\n",
       "      <td>27.8</td>\n",
       "      <td>28.764706</td>\n",
       "      <td>28.766712</td>\n",
       "      <td>61.8</td>\n",
       "      <td>64.470588</td>\n",
       "      <td>63.318843</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.443882</td>\n",
       "      <td>0.453030</td>\n",
       "      <td>7.8</td>\n",
       "      <td>7.117647</td>\n",
       "      <td>8.890309</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.470588</td>\n",
       "      <td>21.488604</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.341353</td>\n",
       "      <td>0.400982</td>\n",
       "      <td>12.2</td>\n",
       "      <td>9.647059</td>\n",
       "      <td>12.555784</td>\n",
       "      <td>17.8</td>\n",
       "      <td>14.352941</td>\n",
       "      <td>17.703171</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.660118</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>8.6</td>\n",
       "      <td>10.588235</td>\n",
       "      <td>9.616473</td>\n",
       "      <td>31.2</td>\n",
       "      <td>33.764706</td>\n",
       "      <td>31.879139</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.647059</td>\n",
       "      <td>17.886353</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.529412</td>\n",
       "      <td>8.602996</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.882353</td>\n",
       "      <td>5.022172</td>\n",
       "      <td>13.4</td>\n",
       "      <td>12.764706</td>\n",
       "      <td>12.977303</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.823529</td>\n",
       "      <td>16.652968</td>\n",
       "      <td>101.36</td>\n",
       "      <td>100.941176</td>\n",
       "      <td>105.335369</td>\n",
       "      <td>106.26</td>\n",
       "      <td>101.882353</td>\n",
       "      <td>103.246985</td>\n",
       "      <td>74.66</td>\n",
       "      <td>72.794118</td>\n",
       "      <td>74.682340</td>\n",
       "      <td>0.2868</td>\n",
       "      <td>0.226765</td>\n",
       "      <td>0.281216</td>\n",
       "      <td>0.3292</td>\n",
       "      <td>0.313882</td>\n",
       "      <td>0.338097</td>\n",
       "      <td>0.5392</td>\n",
       "      <td>0.517706</td>\n",
       "      <td>0.549476</td>\n",
       "      <td>47.62</td>\n",
       "      <td>51.047059</td>\n",
       "      <td>48.916129</td>\n",
       "      <td>56.48</td>\n",
       "      <td>55.852941</td>\n",
       "      <td>60.053062</td>\n",
       "      <td>10.16</td>\n",
       "      <td>10.235294</td>\n",
       "      <td>11.442650</td>\n",
       "      <td>13.38</td>\n",
       "      <td>10.141176</td>\n",
       "      <td>12.989218</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>0.498118</td>\n",
       "      <td>0.522372</td>\n",
       "      <td>16.08</td>\n",
       "      <td>15.305882</td>\n",
       "      <td>15.360195</td>\n",
       "      <td>26.08</td>\n",
       "      <td>31.029412</td>\n",
       "      <td>29.131986</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.151941</td>\n",
       "      <td>0.198241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.6</td>\n",
       "      <td>27.521739</td>\n",
       "      <td>28.995392</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.913043</td>\n",
       "      <td>60.658003</td>\n",
       "      <td>0.4704</td>\n",
       "      <td>0.452478</td>\n",
       "      <td>0.478355</td>\n",
       "      <td>7.8</td>\n",
       "      <td>8.130435</td>\n",
       "      <td>7.027787</td>\n",
       "      <td>20.4</td>\n",
       "      <td>22.173913</td>\n",
       "      <td>17.748878</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.368261</td>\n",
       "      <td>0.400872</td>\n",
       "      <td>16.8</td>\n",
       "      <td>14.869565</td>\n",
       "      <td>15.470900</td>\n",
       "      <td>20.6</td>\n",
       "      <td>19.782609</td>\n",
       "      <td>18.814466</td>\n",
       "      <td>0.8202</td>\n",
       "      <td>0.754478</td>\n",
       "      <td>0.826359</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.869565</td>\n",
       "      <td>8.137672</td>\n",
       "      <td>30.2</td>\n",
       "      <td>31.130435</td>\n",
       "      <td>30.662777</td>\n",
       "      <td>15.4</td>\n",
       "      <td>13.956522</td>\n",
       "      <td>14.745217</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.478261</td>\n",
       "      <td>7.554715</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.608696</td>\n",
       "      <td>2.053649</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.913043</td>\n",
       "      <td>11.026134</td>\n",
       "      <td>14.2</td>\n",
       "      <td>17.217391</td>\n",
       "      <td>14.650023</td>\n",
       "      <td>113.02</td>\n",
       "      <td>106.700000</td>\n",
       "      <td>111.726013</td>\n",
       "      <td>105.28</td>\n",
       "      <td>104.313043</td>\n",
       "      <td>100.707474</td>\n",
       "      <td>70.92</td>\n",
       "      <td>72.026087</td>\n",
       "      <td>71.245725</td>\n",
       "      <td>0.3398</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.314130</td>\n",
       "      <td>0.3338</td>\n",
       "      <td>0.362217</td>\n",
       "      <td>0.293597</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0.555913</td>\n",
       "      <td>0.579132</td>\n",
       "      <td>50.24</td>\n",
       "      <td>49.339130</td>\n",
       "      <td>51.616353</td>\n",
       "      <td>54.14</td>\n",
       "      <td>49.982609</td>\n",
       "      <td>50.929661</td>\n",
       "      <td>9.98</td>\n",
       "      <td>10.182609</td>\n",
       "      <td>10.517400</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.134783</td>\n",
       "      <td>5.521662</td>\n",
       "      <td>0.5346</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.536718</td>\n",
       "      <td>12.38</td>\n",
       "      <td>14.491304</td>\n",
       "      <td>13.639291</td>\n",
       "      <td>26.78</td>\n",
       "      <td>27.121739</td>\n",
       "      <td>27.529568</td>\n",
       "      <td>0.2778</td>\n",
       "      <td>0.247522</td>\n",
       "      <td>0.258858</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.130435</td>\n",
       "      <td>26.754705</td>\n",
       "      <td>57.0</td>\n",
       "      <td>62.217391</td>\n",
       "      <td>56.268072</td>\n",
       "      <td>0.4614</td>\n",
       "      <td>0.453478</td>\n",
       "      <td>0.478241</td>\n",
       "      <td>7.4</td>\n",
       "      <td>8.304348</td>\n",
       "      <td>8.263860</td>\n",
       "      <td>19.2</td>\n",
       "      <td>22.739130</td>\n",
       "      <td>19.162653</td>\n",
       "      <td>0.3888</td>\n",
       "      <td>0.363391</td>\n",
       "      <td>0.429127</td>\n",
       "      <td>22.4</td>\n",
       "      <td>15.956522</td>\n",
       "      <td>22.971298</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.608696</td>\n",
       "      <td>28.785480</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.775304</td>\n",
       "      <td>0.788219</td>\n",
       "      <td>10.6</td>\n",
       "      <td>10.173913</td>\n",
       "      <td>10.900114</td>\n",
       "      <td>32.6</td>\n",
       "      <td>33.260870</td>\n",
       "      <td>33.328730</td>\n",
       "      <td>15.8</td>\n",
       "      <td>16.086957</td>\n",
       "      <td>16.354421</td>\n",
       "      <td>8.8</td>\n",
       "      <td>9.478261</td>\n",
       "      <td>9.189601</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.608696</td>\n",
       "      <td>1.714244</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.869565</td>\n",
       "      <td>13.058401</td>\n",
       "      <td>20.6</td>\n",
       "      <td>19.260870</td>\n",
       "      <td>20.208762</td>\n",
       "      <td>115.12</td>\n",
       "      <td>108.373913</td>\n",
       "      <td>118.242570</td>\n",
       "      <td>108.16</td>\n",
       "      <td>99.195652</td>\n",
       "      <td>104.680081</td>\n",
       "      <td>71.16</td>\n",
       "      <td>73.834783</td>\n",
       "      <td>71.558889</td>\n",
       "      <td>0.5050</td>\n",
       "      <td>0.339957</td>\n",
       "      <td>0.521826</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.364609</td>\n",
       "      <td>0.340520</td>\n",
       "      <td>0.5854</td>\n",
       "      <td>0.558957</td>\n",
       "      <td>0.608281</td>\n",
       "      <td>53.46</td>\n",
       "      <td>51.517391</td>\n",
       "      <td>54.873584</td>\n",
       "      <td>60.72</td>\n",
       "      <td>56.469565</td>\n",
       "      <td>61.325672</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.743478</td>\n",
       "      <td>12.831033</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.095652</td>\n",
       "      <td>4.824586</td>\n",
       "      <td>0.5284</td>\n",
       "      <td>0.520739</td>\n",
       "      <td>0.552592</td>\n",
       "      <td>14.50</td>\n",
       "      <td>15.182609</td>\n",
       "      <td>15.636350</td>\n",
       "      <td>34.22</td>\n",
       "      <td>30.934783</td>\n",
       "      <td>36.304024</td>\n",
       "      <td>0.4064</td>\n",
       "      <td>0.264304</td>\n",
       "      <td>0.418083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FG_SMA     FG_CMA     FG_EMA  FGA_SMA    FGA_CMA    FGA_EMA  FG%_SMA  \\\n",
       "0    24.6  25.000000  26.295223     57.2  58.588235  59.950459   0.4262   \n",
       "1    25.4  25.250000  26.894015     54.8  55.916667  55.720876   0.4620   \n",
       "2    22.6  23.625000  23.893919     57.4  57.750000  57.110197   0.3922   \n",
       "3    27.6  30.210526  27.649443     59.4  61.684211  62.042109   0.4672   \n",
       "4    28.6  27.521739  28.995392     61.0  60.913043  60.658003   0.4704   \n",
       "\n",
       "    FG%_CMA   FG%_EMA  3P_SMA    3P_CMA    3P_EMA  3PA_SMA    3PA_CMA  \\\n",
       "0  0.427588  0.434373     7.4  6.588235  8.204161     23.6  22.176471   \n",
       "1  0.454417  0.480009     7.4  6.250000  7.206811     22.2  19.000000   \n",
       "2  0.408500  0.418203     7.6  7.000000  7.976223     23.8  22.375000   \n",
       "3  0.487421  0.447264     9.2  8.842105  8.484374     26.6  23.736842   \n",
       "4  0.452478  0.478355     7.8  8.130435  7.027787     20.4  22.173913   \n",
       "\n",
       "     3PA_EMA  3P%_SMA   3P%_CMA   3P%_EMA  FT_SMA     FT_CMA     FT_EMA  \\\n",
       "0  24.070312   0.3084  0.299059  0.337075    14.0  12.647059  13.172682   \n",
       "1  20.849769   0.3374  0.326417  0.345893    14.2  14.500000  15.147651   \n",
       "2  22.977595   0.3246  0.315375  0.355482    15.0  15.125000  13.397805   \n",
       "3  27.077445   0.3448  0.362474  0.312032    12.8  13.368421  15.358436   \n",
       "4  17.748878   0.3942  0.368261  0.400872    16.8  14.869565  15.470900   \n",
       "\n",
       "   FTA_SMA    FTA_CMA    FTA_EMA  FT%_SMA   FT%_CMA   FT%_EMA  ORB_SMA  \\\n",
       "0     20.8  19.235294  19.369426   0.6766  0.650882  0.685984      8.0   \n",
       "1     17.4  19.000000  18.813979   0.8158  0.761500  0.804623      5.4   \n",
       "2     18.8  19.750000  17.759031   0.7792  0.751000  0.743282     10.8   \n",
       "3     19.4  19.947368  21.963728   0.6570  0.661421  0.680866     12.2   \n",
       "4     20.6  19.782609  18.814466   0.8202  0.754478  0.826359      8.2   \n",
       "\n",
       "     ORB_CMA    ORB_EMA  TRB_SMA    TRB_CMA    TRB_EMA  AST_SMA    AST_CMA  \\\n",
       "0   8.705882   8.663763     33.0  32.000000  33.494042     15.2  15.058824   \n",
       "1   7.166667   5.983827     29.0  31.083333  31.540353     13.6  11.250000   \n",
       "2  11.250000  10.604938     36.2  36.125000  36.867398     12.2  11.875000   \n",
       "3  11.315789  13.258659     39.2  37.263158  40.132135     16.8  15.894737   \n",
       "4   8.869565   8.137672     30.2  31.130435  30.662777     15.4  13.956522   \n",
       "\n",
       "     AST_EMA  STL_SMA   STL_CMA   STL_EMA  BLK_SMA   BLK_CMA   BLK_EMA  \\\n",
       "0  16.393277      6.0  6.529412  7.253464      4.0  3.529412  3.867263   \n",
       "1  14.659644      8.0  7.166667  7.526371      5.8  6.166667  5.913450   \n",
       "2  13.618656      6.6  7.375000  6.412894      3.4  3.375000  3.427526   \n",
       "3  16.683856      8.0  8.105263  8.039175      4.6  3.421053  4.673308   \n",
       "4  14.745217      7.2  7.478261  7.554715      2.6  1.608696  2.053649   \n",
       "\n",
       "   TOV_SMA    TOV_CMA    TOV_EMA  PF_SMA     PF_CMA     PF_EMA  ORtg_SMA  \\\n",
       "0     12.8  13.588235  12.652723    18.6  19.529412  18.925827     99.60   \n",
       "1     12.8  13.000000  12.254156    16.0  16.416667  14.752449    104.66   \n",
       "2     13.6  15.000000  14.100594    13.8  15.000000  14.414723     98.60   \n",
       "3     10.2  11.631579   9.552080    17.2  15.473684  16.718513    115.44   \n",
       "4     10.0  11.913043  11.026134    14.2  17.217391  14.650023    113.02   \n",
       "\n",
       "     ORtg_CMA    ORtg_EMA  DRtg_SMA    DRtg_CMA    DRtg_EMA  Pace_SMA  \\\n",
       "0   96.376471  102.177531    103.68   99.011765  100.365633     71.06   \n",
       "1  101.841667  108.833516     95.44   92.383333   91.727737     69.40   \n",
       "2   98.512500  100.778692     87.28   89.612500   85.721445     68.42   \n",
       "3  115.610526  115.290975     79.34   88.810526   84.296366     67.32   \n",
       "4  106.700000  111.726013    105.28  104.313043  100.707474     70.92   \n",
       "\n",
       "    Pace_CMA   Pace_EMA  FTr_SMA   FTr_CMA   FTr_EMA  3PAr_SMA  3PAr_CMA  \\\n",
       "0  72.029412  72.268344   0.3742  0.336706  0.333050    0.4134  0.378235   \n",
       "1  69.533333  69.843951   0.3280  0.349167  0.350921    0.4048  0.344750   \n",
       "2  70.312500  68.383082   0.3328  0.345000  0.314520    0.4158  0.386625   \n",
       "3  71.278947  69.146828   0.3318  0.328737  0.355954    0.4474  0.386737   \n",
       "4  72.026087  71.245725   0.3398  0.329000  0.314130    0.3338  0.362217   \n",
       "\n",
       "   3PAr_EMA  TS%_SMA   TS%_CMA   TS%_EMA  TRB%_SMA   TRB%_CMA   TRB%_EMA  \\\n",
       "0  0.402471   0.5246  0.512118  0.532588     51.98  51.194118  53.618324   \n",
       "1  0.374650   0.5724  0.552167  0.587170     48.32  49.283333  50.794579   \n",
       "2  0.402249   0.5122  0.517625  0.528738     54.60  55.950000  55.052309   \n",
       "3  0.435777   0.5668  0.578579  0.550669     59.50  56.963158  58.839409   \n",
       "4  0.293597   0.5796  0.555913  0.579132     50.24  49.339130  51.616353   \n",
       "\n",
       "   AST%_SMA   AST%_CMA   AST%_EMA  STL%_SMA   STL%_CMA   STL%_EMA  BLK%_SMA  \\\n",
       "0     61.28  59.847059  61.491816      8.30   8.976471   9.928151     12.04   \n",
       "1     52.06  43.458333  51.963275     11.52  10.166667  10.706601     15.08   \n",
       "2     55.24  51.325000  57.850754      9.68  10.475000   9.387151      8.18   \n",
       "3     59.84  52.747368  59.355313     11.86  11.331579  11.589912     14.14   \n",
       "4     54.14  49.982609  50.929661      9.98  10.182609  10.517400      6.94   \n",
       "\n",
       "    BLK%_CMA   BLK%_EMA  eFG%_SMA  eFG%_CMA  eFG%_EMA  TOV%_SMA   TOV%_CMA  \\\n",
       "0  10.111765  12.098830    0.4896  0.483588  0.501728     16.10  16.652941   \n",
       "1  17.758333  15.824199    0.5294  0.511917  0.544585     16.82  16.691667   \n",
       "2   8.675000   7.962323    0.4598  0.469750  0.489303     17.04  18.250000   \n",
       "3  10.215789  14.153236    0.5466  0.559684  0.516950     13.04  14.105263   \n",
       "4   5.134783   5.521662    0.5346  0.519000  0.536718     12.38  14.491304   \n",
       "\n",
       "    TOV%_EMA  ORB%_SMA   ORB%_CMA   ORB%_EMA  FT/FGA_SMA  FT/FGA_CMA  \\\n",
       "0  15.522065     25.48  26.558824  27.283188      0.2510    0.221882   \n",
       "1  15.854449     20.80  24.325000  22.854488      0.2676    0.267000   \n",
       "2  17.707910     33.68  34.762500  33.959442      0.2662    0.265250   \n",
       "3  11.757133     38.50  36.057895  39.413917      0.2168    0.218789   \n",
       "4  13.639291     26.78  27.121739  27.529568      0.2778    0.247522   \n",
       "\n",
       "   FT/FGA_EMA  opp_FG_SMA  opp_FG_CMA  opp_FG_EMA  opp_FGA_SMA  opp_FGA_CMA  \\\n",
       "0    0.224880        25.6   26.263158   24.161796         54.6    59.578947   \n",
       "1    0.282660        26.4   25.454545   24.685058         57.0    58.363636   \n",
       "2    0.237552        20.6   20.166667   21.045267         57.0    55.500000   \n",
       "3    0.247359        27.8   28.764706   28.766712         61.8    64.470588   \n",
       "4    0.258858        26.0   28.130435   26.754705         57.0    62.217391   \n",
       "\n",
       "   opp_FGA_EMA  opp_FG%_SMA  opp_FG%_CMA  opp_FG%_EMA  opp_3P_SMA  opp_3P_CMA  \\\n",
       "0    54.535112       0.4650     0.441000     0.440586         7.6    7.368421   \n",
       "1    53.919338       0.4636     0.436000     0.459139         6.0    6.181818   \n",
       "2    57.646091       0.3620     0.364167     0.366605         4.6    4.333333   \n",
       "3    63.318843       0.4500     0.443882     0.453030         7.8    7.117647   \n",
       "4    56.268072       0.4614     0.453478     0.478241         7.4    8.304348   \n",
       "\n",
       "   opp_3P_EMA  opp_3PA_SMA  opp_3PA_CMA  opp_3PA_EMA  opp_3P%_SMA  \\\n",
       "0    6.938694         19.8    20.789474    19.170009       0.3640   \n",
       "1    6.227337         19.4    21.363636    20.010720       0.2982   \n",
       "2    4.946502         15.2    15.666667    15.786008       0.3050   \n",
       "3    8.890309         20.4    20.470588    21.488604       0.3752   \n",
       "4    8.263860         19.2    22.739130    19.162653       0.3888   \n",
       "\n",
       "   opp_3P%_CMA  opp_3P%_EMA  opp_FT_SMA  opp_FT_CMA  opp_FT_EMA  opp_FTA_SMA  \\\n",
       "0     0.353421     0.346890        13.8   12.157895   14.134105         21.6   \n",
       "1     0.291818     0.304787        16.0   15.636364   17.081051         23.2   \n",
       "2     0.282000     0.312971        12.2   11.166667   11.950617         18.0   \n",
       "3     0.341353     0.400982        12.2    9.647059   12.555784         17.8   \n",
       "4     0.363391     0.429127        22.4   15.956522   22.971298         28.0   \n",
       "\n",
       "   opp_FTA_CMA  opp_FTA_EMA  opp_FT%_SMA  opp_FT%_CMA  opp_FT%_EMA  \\\n",
       "0    19.684211    23.175522       0.6444     0.626000     0.617173   \n",
       "1    23.181818    24.057105       0.6832     0.675364     0.709332   \n",
       "2    16.333333    16.748971       0.6736     0.686333     0.719436   \n",
       "3    14.352941    17.703171       0.6736     0.660118     0.696609   \n",
       "4    20.608696    28.785480       0.7750     0.775304     0.788219   \n",
       "\n",
       "   opp_ORB_SMA  opp_ORB_CMA  opp_ORB_EMA  opp_TRB_SMA  opp_TRB_CMA  \\\n",
       "0          9.4    10.947368    10.232045         34.4    36.736842   \n",
       "1          9.2    11.181818     9.276042         32.6    35.636364   \n",
       "2          9.2     8.333333     9.197531         29.8    28.666667   \n",
       "3          8.6    10.588235     9.616473         31.2    33.764706   \n",
       "4         10.6    10.173913    10.900114         32.6    33.260870   \n",
       "\n",
       "   opp_TRB_EMA  opp_AST_SMA  opp_AST_CMA  opp_AST_EMA  opp_STL_SMA  \\\n",
       "0    37.302457         16.4    16.052632    15.467661         11.8   \n",
       "1    31.523480         12.4    13.454545    13.094904          7.6   \n",
       "2    28.954733          9.6     8.833333     9.444444          7.0   \n",
       "3    31.879139         16.0    16.647059    17.886353          7.6   \n",
       "4    33.328730         15.8    16.086957    16.354421          8.8   \n",
       "\n",
       "   opp_STL_CMA  opp_STL_EMA  opp_BLK_SMA  opp_BLK_CMA  opp_BLK_EMA  \\\n",
       "0     9.263158    10.163377          4.6     5.315789     5.158201   \n",
       "1     8.727273     7.716964          7.2     7.545455     6.531423   \n",
       "2     6.666667     7.148148          3.0     2.500000     2.415638   \n",
       "3     7.529412     8.602996          5.2     3.882353     5.022172   \n",
       "4     9.478261     9.189601          1.4     1.608696     1.714244   \n",
       "\n",
       "   opp_TOV_SMA  opp_TOV_CMA  opp_TOV_EMA  opp_PF_SMA  opp_PF_CMA  opp_PF_EMA  \\\n",
       "0         19.2    16.263158    19.961327        19.2   19.315789   19.325415   \n",
       "1         13.4    13.454545    15.302749        19.6   19.272727   20.626886   \n",
       "2         17.8    17.833333    17.629630        18.2   18.833333   16.588477   \n",
       "3         13.4    12.764706    12.977303        17.0   14.823529   16.652968   \n",
       "4         12.0    12.869565    13.058401        20.6   19.260870   20.208762   \n",
       "\n",
       "   opp_ORtg_SMA  opp_ORtg_CMA  opp_ORtg_EMA  opp_DRtg_SMA  opp_DRtg_CMA  \\\n",
       "0         97.32     97.121053     92.661265         81.94     84.326316   \n",
       "1        104.10    102.254545    102.612163         95.84     87.200000   \n",
       "2         78.24     76.233333     79.395473        110.28    111.500000   \n",
       "3        101.36    100.941176    105.335369        106.26    101.882353   \n",
       "4        115.12    108.373913    118.242570        108.16     99.195652   \n",
       "\n",
       "   opp_DRtg_EMA  opp_Pace_SMA  opp_Pace_CMA  opp_Pace_EMA  opp_FTr_SMA  \\\n",
       "0     76.493108         74.54     74.047368     75.133121       0.4000   \n",
       "1     96.516332         72.10     71.272727     70.806744       0.4232   \n",
       "2    114.295062         72.06     71.316667     71.867901       0.3118   \n",
       "3    103.246985         74.66     72.794118     74.682340       0.2868   \n",
       "4    104.680081         71.16     73.834783     71.558889       0.5050   \n",
       "\n",
       "   opp_FTr_CMA  opp_FTr_EMA  opp_3PAr_SMA  opp_3PAr_CMA  opp_3PAr_EMA  \\\n",
       "0     0.335579     0.429435        0.3626      0.350158      0.351612   \n",
       "1     0.408545     0.460333        0.3428      0.370273      0.375382   \n",
       "2     0.287667     0.282848        0.2690      0.286667      0.278337   \n",
       "3     0.226765     0.281216        0.3292      0.313882      0.338097   \n",
       "4     0.339957     0.521826        0.3386      0.364609      0.340520   \n",
       "\n",
       "   opp_TS%_SMA  opp_TS%_CMA  opp_TS%_EMA  opp_TRB%_SMA  opp_TRB%_CMA  \\\n",
       "0       0.5588     0.523474     0.528957         56.72     55.089474   \n",
       "1       0.5518     0.526000     0.558821         52.18     53.881818   \n",
       "2       0.4448     0.443000     0.452086         46.94     45.100000   \n",
       "3       0.5392     0.517706     0.549476         47.62     51.047059   \n",
       "4       0.5854     0.558957     0.608281         53.46     51.517391   \n",
       "\n",
       "   opp_TRB%_EMA  opp_AST%_SMA  opp_AST%_CMA  opp_AST%_EMA  opp_STL%_SMA  \\\n",
       "0     58.697837         65.74     61.347368     65.274972         15.62   \n",
       "1     53.677695         48.30     53.681818     54.299240         10.64   \n",
       "2     45.559671         45.88     42.866667     44.186831          9.24   \n",
       "3     48.916129         56.48     55.852941     60.053062         10.16   \n",
       "4     54.873584         60.72     56.469565     61.325672         12.40   \n",
       "\n",
       "   opp_STL%_CMA  opp_STL%_EMA  opp_BLK%_SMA  opp_BLK%_CMA  opp_BLK%_EMA  \\\n",
       "0     12.384211     13.507466         12.94     15.321053     14.292547   \n",
       "1     12.181818     10.918891         17.34     18.818182     16.227338   \n",
       "2      8.933333      9.470782          8.90      7.416667      7.004527   \n",
       "3     10.235294     11.442650         13.38     10.141176     12.989218   \n",
       "4     12.743478     12.831033          4.02      5.095652      4.824586   \n",
       "\n",
       "   opp_eFG%_SMA  opp_eFG%_CMA  opp_eFG%_EMA  opp_TOV%_SMA  opp_TOV%_CMA  \\\n",
       "0        0.5340      0.503316      0.503701         22.70     19.210526   \n",
       "1        0.5164      0.489273      0.517752         16.48     16.400000   \n",
       "2        0.4028      0.403333      0.409926         21.58     22.283333   \n",
       "3        0.5128      0.498118      0.522372         16.08     15.305882   \n",
       "4        0.5284      0.520739      0.552592         14.50     15.182609   \n",
       "\n",
       "   opp_TOV%_EMA  opp_ORB%_SMA  opp_ORB%_CMA  opp_ORB%_EMA  opp_FT/FGA_SMA  \\\n",
       "0     23.150352         32.98     32.400000     33.972093          0.2552   \n",
       "1     19.017409         32.02     35.209091     35.178476          0.2930   \n",
       "2     21.498765         26.84     24.750000     26.860494          0.2110   \n",
       "3     15.360195         26.08     31.029412     29.131986          0.1958   \n",
       "4     15.636350         34.22     30.934783     36.304024          0.4064   \n",
       "\n",
       "   opp_FT/FGA_CMA  opp_FT/FGA_EMA  Neutral  Win  Loss  \n",
       "0        0.206421        0.261465        0    0     1  \n",
       "1        0.277182        0.328098        0    0     1  \n",
       "2        0.196667        0.202350        0    1     0  \n",
       "3        0.151941        0.198241        0    1     0  \n",
       "4        0.264304        0.418083        0    0     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.abspath(f'../../data/testing_set.csv')\n",
    "testing_df = pd.read_csv(path)\n",
    "testing_df['Loss'] = 1 - testing_df['Win']\n",
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12847 train examples\n",
      "5507 test examples\n"
     ]
    }
   ],
   "source": [
    "train_win_true, test_win_true = training_df.pop('Win'), testing_df.pop('Win')\n",
    "train_loss_true, test_loss_true = training_df.pop('Loss'), testing_df.pop('Loss')\n",
    "\n",
    "print(f'{len(training_df)} train examples')\n",
    "print(f'{len(testing_df)} test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest model\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'loss': ['log_loss', 'deviance', 'exponential'],  # Loss function\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate (eta)\n",
    "    'max_depth': [3, 5, 8],  # Maximum depth of each tree\n",
    "    'max_features': ['log2', 'sqrt'],  # Maximum number of features considered for splitting\n",
    "    'n_estimators': [100, 200, 300]  # Number of trees\n",
    "}\n",
    "\n",
    "# Create a single split\n",
    "cv_single_split = ShuffleSplit(n_splits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Win as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 162 candidates, totalling 162 fits\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=0.637 total time=   2.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=200;, score=0.665 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=300;, score=0.683 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.645 total time=   3.6s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.669 total time=   7.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.695 total time=  10.6s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=0.650 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=200;, score=0.686 total time=   6.7s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=300;, score=0.693 total time=  10.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.661 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.686 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.700 total time=  18.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100;, score=0.661 total time=   6.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=log2, n_estimators=200;, score=0.684 total time=  12.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=log2, n_estimators=300;, score=0.697 total time=  18.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.675 total time=  10.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.694 total time=  20.6s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.697 total time=  31.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=log2, n_estimators=100;, score=0.633 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=log2, n_estimators=200;, score=0.664 total time=   4.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=log2, n_estimators=300;, score=0.684 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.637 total time=   3.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.675 total time=   7.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.690 total time=  10.7s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=log2, n_estimators=100;, score=0.657 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=log2, n_estimators=200;, score=0.686 total time=   6.7s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=log2, n_estimators=300;, score=0.687 total time=  10.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.661 total time=   6.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.694 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.697 total time=  18.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=log2, n_estimators=100;, score=0.664 total time=   6.2s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=log2, n_estimators=200;, score=0.681 total time=  12.5s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=log2, n_estimators=300;, score=0.690 total time=  18.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.672 total time=  10.5s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.693 total time=  21.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.704 total time=  31.2s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=0.690 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=log2, n_estimators=200;, score=0.703 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=log2, n_estimators=300;, score=0.700 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.706 total time=   3.5s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.705 total time=   7.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.707 total time=  10.5s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=0.690 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=log2, n_estimators=200;, score=0.692 total time=   6.6s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=log2, n_estimators=300;, score=0.693 total time=   9.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.694 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.697 total time=  11.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.695 total time=  17.6s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100;, score=0.695 total time=   6.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=log2, n_estimators=200;, score=0.699 total time=  11.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=log2, n_estimators=300;, score=0.689 total time=  17.7s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.700 total time=  10.2s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.688 total time=  20.3s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.697 total time=  30.1s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=log2, n_estimators=100;, score=0.687 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=log2, n_estimators=200;, score=0.696 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=log2, n_estimators=300;, score=0.695 total time=   6.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.705 total time=   3.5s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.704 total time=   6.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.709 total time=  10.5s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=log2, n_estimators=100;, score=0.695 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=log2, n_estimators=200;, score=0.691 total time=   6.7s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=log2, n_estimators=300;, score=0.700 total time=   9.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.704 total time=   6.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.708 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.702 total time=  17.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=log2, n_estimators=100;, score=0.701 total time=   6.1s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=log2, n_estimators=200;, score=0.696 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=log2, n_estimators=300;, score=0.689 total time=  17.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.696 total time=  10.2s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.691 total time=  20.3s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.686 total time=  30.3s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=0.694 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=200;, score=0.688 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=300;, score=0.700 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.692 total time=   3.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.700 total time=   6.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.701 total time=  10.6s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=0.689 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=200;, score=0.679 total time=   6.6s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=300;, score=0.696 total time=   9.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.703 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.704 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.696 total time=  17.7s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100;, score=0.681 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=200;, score=0.681 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=300;, score=0.688 total time=  17.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.681 total time=  10.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.684 total time=  20.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.673 total time=  30.5s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=log2, n_estimators=100;, score=0.704 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=log2, n_estimators=200;, score=0.689 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=log2, n_estimators=300;, score=0.699 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.695 total time=   3.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.699 total time=   6.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.693 total time=  10.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=log2, n_estimators=100;, score=0.692 total time=   3.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=log2, n_estimators=200;, score=0.690 total time=   6.5s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=log2, n_estimators=300;, score=0.684 total time=   9.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.702 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.701 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.689 total time=  17.6s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=log2, n_estimators=100;, score=0.682 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=log2, n_estimators=200;, score=0.689 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=log2, n_estimators=300;, score=0.676 total time=  17.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.672 total time=  10.1s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.693 total time=  20.3s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.690 total time=  30.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "54 fits failed out of a total of 162.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "54 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'exponential', 'log_loss'}. Got 'deviance' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.63735409 0.66459144 0.68326848 0.64513619 0.6692607  0.69494163\n",
      " 0.64980545 0.68560311 0.69338521 0.66070039 0.68560311 0.69961089\n",
      " 0.66070039 0.68404669 0.69727626 0.67470817 0.69416342 0.69727626\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63346304 0.66381323 0.68404669 0.63657588 0.67548638 0.69027237\n",
      " 0.65680934 0.68560311 0.68715953 0.6614786  0.69416342 0.69727626\n",
      " 0.66381323 0.68093385 0.69027237 0.67237354 0.69338521 0.70350195\n",
      " 0.69027237 0.70272374 0.70038911 0.70583658 0.70505837 0.70661479\n",
      " 0.69027237 0.69182879 0.69338521 0.69416342 0.69727626 0.69494163\n",
      " 0.69494163 0.69883268 0.68949416 0.69961089 0.68793774 0.69727626\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68715953 0.69649805 0.69494163 0.70505837 0.70350195 0.70894942\n",
      " 0.69494163 0.69105058 0.70038911 0.70428016 0.70817121 0.70194553\n",
      " 0.70116732 0.69571984 0.68871595 0.69649805 0.69105058 0.68638132\n",
      " 0.69416342 0.68793774 0.69961089 0.69182879 0.69961089 0.70116732\n",
      " 0.68871595 0.67859922 0.69571984 0.70272374 0.70350195 0.69571984\n",
      " 0.68093385 0.68093385 0.68793774 0.68093385 0.68404669 0.67315175\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70428016 0.68871595 0.69883268 0.69494163 0.69883268 0.692607\n",
      " 0.69182879 0.69027237 0.68404669 0.70194553 0.70116732 0.68949416\n",
      " 0.68249027 0.68949416 0.67626459 0.67237354 0.692607   0.69027237]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.05, 'loss': 'exponential', 'max_depth': 3, 'max_features': 'sqrt', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Create the grid search object\n",
    "grid_search_win = GridSearchCV(gbc, param_grid, cv=cv_single_split, verbose=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_win.fit(training_df, train_win_true)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search_win.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.11\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.47      0.54      2180\n",
      "           1       0.70      0.82      0.76      3327\n",
      "\n",
      "    accuracy                           0.68      5507\n",
      "   macro avg       0.67      0.65      0.65      5507\n",
      "weighted avg       0.67      0.68      0.67      5507\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1029 1151]\n",
      " [ 605 2722]]\n"
     ]
    }
   ],
   "source": [
    "# Acess the best model\n",
    "clf = grid_search_win.best_estimator_\n",
    "\n",
    "y_pred = clf.predict(testing_df)\n",
    "\n",
    "accuracy = accuracy_score(test_win_true, y_pred)\n",
    "print(f\"Accuracy: {(accuracy*100):.2f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_win_true, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_win_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Loss as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 162 candidates, totalling 162 fits\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=0.613 total time=   2.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=200;, score=0.654 total time=   4.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=300;, score=0.665 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.627 total time=   3.5s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.654 total time=   7.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.668 total time=  10.5s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=0.640 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=200;, score=0.662 total time=   6.7s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=300;, score=0.676 total time=  10.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.652 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.675 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.670 total time=  17.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100;, score=0.661 total time=   6.2s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=log2, n_estimators=200;, score=0.665 total time=  12.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=log2, n_estimators=300;, score=0.670 total time=  18.3s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.662 total time=  10.5s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.668 total time=  20.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.674 total time=  30.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=log2, n_estimators=100;, score=0.618 total time=   2.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=log2, n_estimators=200;, score=0.651 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=log2, n_estimators=300;, score=0.665 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.622 total time=   3.5s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.658 total time=   7.0s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.671 total time=  10.6s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=log2, n_estimators=100;, score=0.643 total time=   3.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=log2, n_estimators=200;, score=0.667 total time=   6.7s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=log2, n_estimators=300;, score=0.669 total time=  10.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.647 total time=   6.1s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.668 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.675 total time=  17.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=log2, n_estimators=100;, score=0.654 total time=   6.2s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=log2, n_estimators=200;, score=0.667 total time=  12.5s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=log2, n_estimators=300;, score=0.673 total time=  18.6s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.663 total time=  10.4s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.665 total time=  20.9s\n",
      "[CV 1/1] END learning_rate=0.01, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.668 total time=  31.2s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=0.676 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=log2, n_estimators=200;, score=0.676 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=log2, n_estimators=300;, score=0.683 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.678 total time=   3.5s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.672 total time=   7.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.681 total time=  10.4s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=0.656 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=log2, n_estimators=200;, score=0.668 total time=   6.6s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=log2, n_estimators=300;, score=0.672 total time=   9.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.675 total time=   6.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.675 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.665 total time=  17.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100;, score=0.675 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=log2, n_estimators=200;, score=0.652 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=log2, n_estimators=300;, score=0.665 total time=  17.5s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.666 total time=  10.1s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.668 total time=  20.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.667 total time=  29.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=log2, n_estimators=100;, score=0.668 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=log2, n_estimators=200;, score=0.674 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=log2, n_estimators=300;, score=0.689 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.682 total time=   3.5s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.677 total time=   6.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.686 total time=  10.4s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=log2, n_estimators=100;, score=0.672 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=log2, n_estimators=200;, score=0.672 total time=   6.6s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=log2, n_estimators=300;, score=0.666 total time=   9.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.675 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.676 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.677 total time=  17.6s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=log2, n_estimators=100;, score=0.674 total time=   6.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=log2, n_estimators=200;, score=0.666 total time=  11.9s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=log2, n_estimators=300;, score=0.661 total time=  17.7s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.682 total time=  10.2s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.665 total time=  20.0s\n",
      "[CV 1/1] END learning_rate=0.05, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.663 total time=  30.2s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=0.672 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=200;, score=0.678 total time=   3.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=300;, score=0.672 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.676 total time=   3.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.674 total time=   6.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.682 total time=  10.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=0.685 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=200;, score=0.669 total time=   6.5s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=300;, score=0.661 total time=   9.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.669 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.665 total time=  11.6s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.665 total time=  17.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100;, score=0.649 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=200;, score=0.669 total time=  11.6s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=300;, score=0.656 total time=  17.5s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.660 total time=   9.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.651 total time=  19.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.640 total time=  29.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=3, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=5, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=log2, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=log2, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=log2, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=deviance, max_depth=8, max_features=sqrt, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=log2, n_estimators=100;, score=0.673 total time=   1.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=log2, n_estimators=200;, score=0.678 total time=   3.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=log2, n_estimators=300;, score=0.673 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=100;, score=0.672 total time=   3.5s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=200;, score=0.680 total time=   6.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=3, max_features=sqrt, n_estimators=300;, score=0.686 total time=  10.4s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=log2, n_estimators=100;, score=0.681 total time=   3.3s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=log2, n_estimators=200;, score=0.666 total time=   6.5s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=log2, n_estimators=300;, score=0.659 total time=   9.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=100;, score=0.677 total time=   5.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=200;, score=0.672 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=5, max_features=sqrt, n_estimators=300;, score=0.676 total time=  17.5s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=log2, n_estimators=100;, score=0.658 total time=   5.9s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=log2, n_estimators=200;, score=0.658 total time=  11.7s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=log2, n_estimators=300;, score=0.662 total time=  17.8s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=100;, score=0.650 total time=  10.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=200;, score=0.653 total time=  20.0s\n",
      "[CV 1/1] END learning_rate=0.1, loss=exponential, max_depth=8, max_features=sqrt, n_estimators=300;, score=0.669 total time=  30.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "54 fits failed out of a total of 162.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "54 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'exponential', 'log_loss'}. Got 'deviance' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\mtayl\\anaconda3\\envs\\mlmb-utils\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.61322957 0.6536965  0.66536965 0.62723735 0.6536965  0.66848249\n",
      " 0.63968872 0.66225681 0.67626459 0.65214008 0.67548638 0.67003891\n",
      " 0.66070039 0.66536965 0.67003891 0.66225681 0.66848249 0.67392996\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.61789883 0.65058366 0.66536965 0.62178988 0.65836576 0.67081712\n",
      " 0.64280156 0.66692607 0.6692607  0.64747082 0.66770428 0.67548638\n",
      " 0.65447471 0.66692607 0.67315175 0.66303502 0.66536965 0.66848249\n",
      " 0.67626459 0.67626459 0.68326848 0.67782101 0.67159533 0.68093385\n",
      " 0.65603113 0.66848249 0.67159533 0.67470817 0.67470817 0.66459144\n",
      " 0.67470817 0.65214008 0.66459144 0.66614786 0.66770428 0.66692607\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66848249 0.67392996 0.68871595 0.68171206 0.6770428  0.68638132\n",
      " 0.67237354 0.67237354 0.66614786 0.67470817 0.67626459 0.6770428\n",
      " 0.67392996 0.66614786 0.6614786  0.68171206 0.66459144 0.66303502\n",
      " 0.67237354 0.67782101 0.67237354 0.67626459 0.67392996 0.68171206\n",
      " 0.6848249  0.6692607  0.6614786  0.6692607  0.66459144 0.66459144\n",
      " 0.64902724 0.6692607  0.65603113 0.65992218 0.65136187 0.64046693\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67315175 0.67782101 0.67315175 0.67237354 0.68015564 0.68560311\n",
      " 0.68093385 0.66614786 0.65914397 0.6770428  0.67237354 0.67626459\n",
      " 0.65836576 0.65836576 0.66225681 0.64980545 0.65291829 0.6692607 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.05, 'loss': 'exponential', 'max_depth': 3, 'max_features': 'log2', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Create the grid search object\n",
    "grid_search_loss = GridSearchCV(gbc, param_grid, cv=cv_single_split, verbose=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_loss.fit(training_df, train_loss_true)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search_loss.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.60\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76      3327\n",
      "           1       0.64      0.48      0.55      2180\n",
      "\n",
      "    accuracy                           0.69      5507\n",
      "   macro avg       0.67      0.65      0.65      5507\n",
      "weighted avg       0.68      0.69      0.68      5507\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2742  585]\n",
      " [1144 1036]]\n"
     ]
    }
   ],
   "source": [
    "# Acess the best model\n",
    "clf = grid_search_loss.best_estimator_\n",
    "\n",
    "y_pred = clf.predict(testing_df)\n",
    "\n",
    "accuracy = accuracy_score(test_loss_true, y_pred)\n",
    "print(f\"Accuracy: {(accuracy*100):.2f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_loss_true, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_loss_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for pair in [('win', grid_search_win.best_estimator_), ('loss', grid_search_loss.best_estimator_)]:\n",
    "    filename = f'gradient_boosting_{pair[0]}_model.pkl'\n",
    "    path = os.path.abspath(f'../../machine-learning/model/{filename}')\n",
    "    pickle.dump(pair[1], open(path, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmb-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
